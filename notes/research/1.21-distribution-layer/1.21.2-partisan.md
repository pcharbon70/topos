# Partisan: A Scalable Distribution Layer for Erlang/BEAM

Partisan is a high-performance alternative distribution layer for the BEAM virtual machine that enables Erlang and Elixir applications to scale from hundreds to thousands of nodes—delivering **up to 38x throughput improvements** over standard Distributed Erlang. Created by Christopher Meiklejohn at Carnegie Mellon University and now maintained by Alejandro Ramallo at Leapsight, Partisan replaces Distributed Erlang's fixed full-mesh topology with pluggable overlay networks that can be selected at runtime. The library is production-proven, handling over **30 million GPS transmissions daily** across 300,000 vehicles at LO/JACK LATAM, and currently powers large-scale IoT platforms and distributed databases throughout the BEAM ecosystem.

Partisan's core innovation lies in exposing network topology control to application developers while preserving the familiar actor-based programming model. By separating concerns—topology from application logic, control plane from data plane, membership from routing—Partisan achieves order-of-magnitude performance improvements while enabling unprecedented scale. The system is actively maintained with the stable v5.0.2 release as of August 2025, featuring comprehensive OTP behavior support and commercial backing.

## Fundamental architecture: pluggable topologies replace fixed mesh

Partisan addresses a critical limitation in Distributed Erlang: the mandatory full-mesh topology that creates **O(n²) connections** and restricts practical clusters to 60-200 nodes. Rather than hardcoding network structure in the VM, Partisan implements a pluggable peer service manager architecture where developers select from multiple overlay topologies at runtime without changing application code.

The architecture centers on **four primary topology backends**. The `partisan_pluggable_peer_service_manager` provides full-mesh connectivity similar to Distributed Erlang but with enhanced features like multiple channels and parallelism, scaling to approximately 200 nodes. The `partisan_hyparview_peer_service_manager` implements a modified HyParView protocol for peer-to-peer partial mesh topologies, enabling clusters of **2,000+ nodes** by maintaining only log(n) + c connections per node rather than full connectivity. The `partisan_client_server_peer_service_manager` creates star topologies suitable for mobile and IoT applications where clients connect only to servers while servers mesh among themselves, supporting roughly 1,000 nodes. Finally, `partisan_static_peer_service_manager` provides explicitly configured connections for fixed infrastructure deployments.

Each backend implements the same `membership_strategy` interface, requiring only internal state transitions and updated member lists from developers while Partisan handles connection management, message serialization using `term_to_binary`, failure detection via TCP, and message forwarding. This "bring your own overlay" design means implementing custom protocols requires minimal code—the reference full-mesh implementation comprises just **152 lines of code**—while Partisan manages all low-level networking concerns automatically.

The separation extends to a Software-Defined Networking (SDN) approach with distinct control and data planes. The control plane manages network policies, topology configuration, and membership decisions, while the data plane handles actual packet forwarding and message delivery. This separation enables dynamic network adjustments without application restarts and allows different traffic types to flow over separate channels with individual tuning parameters.

## Named channels eliminate head-of-line blocking

A critical innovation in Partisan's architecture is **named channels**—logical groupings of one or more physical TCP connections between nodes that eliminate the single-connection bottleneck in Distributed Erlang. Where the BEAM's default distribution layer multiplexes all communication through one TCP connection per node pair, causing head-of-line blocking when large messages or slow background processes interfere with latency-sensitive traffic, Partisan allows applications to define multiple channels with independent characteristics.

Each channel can be configured with a **parallelism setting** that specifies how many TCP connections compose that channel, allowing load distribution across connections while preserving message ordering when needed. For example, a typical configuration might define a `control` channel with parallelism of 1 for low-latency coordination messages, a `data` channel with parallelism of 8 for high-throughput application traffic, and a `state_sync` channel with parallelism of 2 configured as monotonic for CRDT state propagation. The system uses affinitized scheduling to partition messages across channel connections based on source-destination process identifiers, maintaining ordering where required while exploiting concurrency.

**Monotonic channels** represent a particularly clever optimization for eventually consistent systems. When enabled, these channels automatically drop intermediate messages when newer state is available—ideal for propagating monotonically increasing data structures like CRDTs, vector clocks, or hash rings. Under overload conditions, monotonic channels provide automatic load shedding by discarding obsolete updates without manual intervention. This feature alone can dramatically reduce bandwidth consumption in gossip-heavy applications.

Performance measurements demonstrate the power of this approach. In Riak Core benchmarks with traffic separated across three channels (requests, metadata anti-entropy, and ring gossip), Partisan achieved **12.5x improvement** over Distributed Erlang. With 8 parallel connections and 1MB payloads, the system delivered **18x improvement under normal conditions and 30x improvement under network congestion and high concurrency**. The key insight is that different message types benefit from different network characteristics—control messages need low latency, data messages need high throughput, and state synchronization needs intelligent coalescing.

## HyParView membership protocol enables massive scale

The HyParView peer service manager represents Partisan's most sophisticated topology option, implementing a hybrid partial view membership protocol designed for environments with thousands of nodes and high churn rates. Unlike full-mesh topologies where every node maintains connections to all others, HyParView nodes maintain **two distinct partial views** of the cluster with fundamentally different properties and maintenance strategies.

The **active view** comprises a small symmetric set of nodes—typically 6 members, or roughly log(n) + c where n is cluster size—with whom the node maintains open TCP connections. This forms a connected overlay graph used for direct message dissemination. The symmetric property ensures that if node q appears in node p's active view, then p appears in q's active view, creating bidirectional communication paths. Active view connections are continuously tested at each broadcast operation, enabling failure detection within seconds rather than the minutes required by heartbeat-based approaches.

The **passive view** maintains a larger backup list—default 30 nodes—of potential communication partners discovered through gossip but without active connections. When active view members fail, passive view nodes are immediately promoted to fill gaps, enabling rapid recovery from failures. The passive view is maintained through periodic shuffle operations every 10 seconds where nodes exchange random subsets of their views, and through random walk propagation when new nodes join. This dual-view architecture combines the efficiency of small active sets with the resilience of large passive reservoirs.

Message routing in HyParView uses the **Plumtree protocol** (Epidemic Broadcast Trees) to construct spanning trees over the partial mesh overlay. When a node needs to send a message to a non-adjacent node, Plumtree computes efficient routing paths through intermediate nodes while maintaining reliability guarantees. The protocol continuously optimizes the spanning tree structure through lazy push and eager push mechanisms, balancing low latency with resilience to failures. Research shows HyParView can sustain **up to 90% node failures** while maintaining cluster connectivity—a remarkable resilience property impossible in full-mesh topologies where such failures would isolate survivors.

The trade-offs are significant and application-specific. HyParView sacrifices FIFO message ordering guarantees because messages between non-adjacent nodes may take different paths through the spanning tree. It provides probabilistic rather than strong membership consistency—different nodes may temporarily have inconsistent views of the cluster. These properties make HyParView unsuitable for strong consistency protocols like Raft or Paxos that require stable membership, but ideal for eventually consistent systems, CRDT-based storage, and AP (Available, Partition-tolerant) architectures where high availability under partitions matters more than strong consistency.

## Programming model: familiar APIs with explicit distribution control

Partisan preserves the actor-based programming model familiar to Erlang developers while exposing explicit control over distribution. The API consolidates around two primary modules—`partisan` and `partisan_peer_service`—that mirror Erlang's built-in distribution primitives with extensions for topology control.

The `partisan` module serves as a drop-in replacement for `erlang` and `net_kernel` functions. Developers use `partisan:send/2` instead of the `!` operator, `partisan:nodes/0` to list cluster members, and `partisan:spawn/2` to create remote processes. Key differences appear in the ability to specify channels and options: `partisan:forward_message(Node, Channel, RemotePid, Message, Options)` allows routing through named channels with features like acknowledgment (`ack => true`), causal delivery (`causal_label => Label`), or broadcast mode (`broadcast => true`). The options map provides fine-grained control over message semantics without changing application structure.

Process identifiers require special handling because PIDs in Erlang are VM-relative. Partisan introduces **remote references** encoded as `partisan_remote_ref` structures that encapsulate node information. Three encoding formats offer memory-latency trade-offs: URI format (`<<"partisan:pid:node@host:0.123.0">>`) for compact representation, improper list format for balanced performance (the default), and tuple format for lowest latency at highest memory cost. A parse transform provides semi-automatic conversion from Distributed Erlang code to Partisan equivalents, though some manual adaptation remains necessary.

OTP behaviors receive full support through Partisan-specific implementations. The `partisan_gen_server` behavior works identically to `gen_server` but operates over Partisan's distribution layer, supporting remote calls via `partisan_gen_server:call({Node, ServerName}, Request)` and casts via `partisan_gen_server:cast({Node, ServerName}, Message)`. Developers can specify which channel a gen_server uses when starting: `partisan_gen_server:start_link(ServerName, Module, Args, [{channel, data_channel}])`. Similar behaviors exist for `partisan_gen_statem`, `partisan_gen_event`, and supporting modules like `partisan_proc_lib` and `partisan_sys`, all tested against adapted Erlang/OTP Common Test suites.

Cluster formation requires explicit node specification rather than magical discovery. Applications obtain their own node spec via `partisan:node_spec()` which returns a map containing name, listen addresses (IP and port), channel configurations, and custom metadata. Joining a cluster means calling `partisan_peer_service:join(RemoteSpec)` with another node's specification. This explicit approach trades convenience for transparency—developers understand exactly how nodes connect and can debug networking issues more easily than with Distributed Erlang's opaque connection logic.

Monitoring and failure callbacks integrate seamlessly. Developers register connection callbacks with `partisan_peer_service:on_up(Node, Fun)` and `partisan_peer_service:on_down(Node, Fun)` to receive notifications when nodes connect or disconnect, optionally specifying which channel to monitor. Process monitoring works through `partisan:monitor(process, RemotePid)` returning a reference that delivers `{'DOWN', Ref, process, RemotePid, Reason}` messages on failure, though this feature currently works only with the full-mesh backend due to the complexity of tracking arbitrary processes across partial view topologies.

## Failure detection: TCP-based implicit approach replaces heartbeats

Partisan fundamentally reimagines failure detection by using **TCP connection state as an implicit failure detector** rather than Distributed Erlang's explicit heartbeat protocol. In Distributed Erlang, nodes send periodic heartbeat messages controlled by the `net_ticktime` parameter (default 60 seconds), declaring peers failed if heartbeats miss within the ticktime window ±25%. This creates **45-750 second failure detection windows** depending on configuration and consumes significant bandwidth in large clusters due to O(n²) heartbeat traffic in full-mesh topologies.

Partisan's approach considers nodes failed when TCP connections drop, detected immediately by the operating system's TCP stack. This provides **seconds-to-minutes detection latency** rather than the minute-scale delays in heartbeat systems. In HyParView topologies, detection is even faster because every broadcast operation tests all active view connections, enabling sub-second failure awareness when traffic is frequent. The trade-off is that TCP-based detection can generate false positives during transient network congestion or when middleboxes aggressively close idle connections, requiring applications to tolerate occasional spurious disconnections.

The system configures connection behavior through several parameters. The `connection_interval` setting controls time between connection attempts (default 1000ms), with `connection_jitter` adding randomness to prevent thundering herd effects. Connection pinging can be enabled to maintain TCP keepalives on idle connections, preventing aggressive middlebox timeouts. These settings allow tuning for different network environments—low-latency datacenters can use aggressive detection while high-latency or unreliable networks benefit from more conservative configurations.

Recovery mechanisms vary by topology. In full-mesh mode, Partisan simply attempts to re-establish TCP connections to all known peers, eventually restoring full connectivity after transient failures. HyParView implements sophisticated recovery through its dual-view architecture: when active view members fail, nodes immediately select random passive view candidates, attempt TCP connections, and send neighbor requests with priority levels. High-priority requests are sent when the active view is empty (critical situation), low-priority otherwise. On acceptance, passive nodes are promoted to active, and the passive view is replenished through continued shuffling and random walks.

**Plumtree tree repair** provides additional resilience in HyParView clusters. The protocol maintains spanning trees for broadcast dissemination, and when tree links fail, Plumtree uses lazy push/eager push mechanisms to rapidly reconstruct efficient paths. Simulations demonstrate recovery from **50% node failures with minimal service disruption**, and the system remains operational even at 90% failure rates, though with degraded performance. This resilience far exceeds what's possible in full-mesh topologies where equivalent failures would fragment the cluster into isolated islands.

Network partition handling embraces eventual consistency rather than attempting strong partition tolerance. HyParView nodes separated by network partitions continue operating independently, forming separate subclusters without explicit partition detection. The protocol provides **probabilistic membership guarantees**—different nodes may have inconsistent views of cluster composition at any moment—but ensures eventual connectivity once partitions heal. Membership convergence typically occurs within minutes through continued gossip and shuffle operations. This model suits eventually consistent systems like CRDT databases but not strongly consistent consensus protocols requiring stable membership views.

## Performance characteristics: order-of-magnitude improvements demonstrated

Extensive benchmarking reveals Partisan's performance advantages across multiple dimensions. At baseline with single channels, Partisan performs **on par with or slightly better than** Distributed Erlang despite being a user-space library rather than VM-embedded functionality—a remarkable achievement demonstrating efficient implementation. The real gains appear when exploiting Partisan's advanced features.

Multi-channel configurations deliver dramatic improvements. In Riak Core benchmarks with three-node clusters, separating request traffic, metadata anti-entropy, and ring gossip onto distinct channels yielded **12.5x average improvement** with best-case scenarios reaching higher multiples. The performance gain stems from eliminating head-of-line blocking where large anti-entropy transfers previously blocked latency-sensitive requests on the single Distributed Erlang connection. Applications with distinct traffic types—control plane coordination, data plane transfers, background maintenance—benefit proportionally to how well these can be segregated onto specialized channels.

Parallel connections scale performance nearly linearly under high concurrency. With **8 parallel connections per channel and 1MB payloads**, benchmarks showed **18x improvement under normal conditions and 30x improvement under network congestion and high concurrency**. The key insight is that low-concurrency scenarios see minimal benefit since single connections suffice for light loads, but high-throughput applications with many concurrent message streams fully exploit parallel connections. Testing of throughput-intensive Riak Core key-value workloads with 1:1 get/put ratios, 10,000 normally-distributed keys, and 1MB objects demonstrated **order-of-magnitude throughput increases**, with latency reductions approaching two orders of magnitude as network latency increased.

Topology-specific optimizations yield application-dependent benefits. Experiments show **up to 38.07x throughput increase** and **13.5x latency reduction** in scenarios where topology matches application communication patterns. For example, hub-and-spoke applications using client-server topology avoid unnecessary peer-to-peer connections, while fully-distributed systems using HyParView reduce per-node connection overhead from O(n) to O(log n) connections, enabling proportional scale increases.

Scalability testing validates theoretical predictions. Distributed Erlang clusters practically limit at **60-200 nodes** depending on application characteristics—Ericsson's largest known deployment reached approximately 200 nodes—due to connection count, heartbeat overhead, and global operation costs. Partisan's full-mesh mode comfortably scales to hundreds of nodes by eliminating heartbeat traffic and supporting multiple connections. The Lasp distributed programming language using Partisan successfully demonstrated **1,024-node clusters** on Amazon EC2 m3.2xlarge instances, though client-server topology failed above 256 nodes. HyParView topology is theoretically proven to **2,000+ nodes** and production deployments at Bondy target this scale.

Real-world production metrics from LO/JACK LATAM validate performance claims. The vehicle tracking platform handles **300,000 vehicles, 10,000 connected devices, and 30+ million GPS transmissions daily** using Partisan as the transport layer for Erleans virtual actors and PlumDB state storage. The system has operated reliably since 2019 across geographically distributed Latin American deployments with varying network quality—a demanding environment that would challenge traditional Distributed Erlang architectures.

## Use cases: IoT platforms, geo-distribution, and large-scale systems

Partisan excels in scenarios where Distributed Erlang's limitations become constraints. The primary use case is **cluster size exceeding 60-200 nodes**, where full-mesh topologies become untenable. Applications planning horizontal scaling to hundreds or thousands of nodes—microservice meshes, IoT platforms, distributed databases—benefit from switching to Partisan's peer-to-peer or client-server topologies before hitting Distributed Erlang's wall.

**Geographic distribution** represents another sweet spot. Distributed Erlang assumes low-latency datacenter networking with reliable connections and uniform latency between all nodes. Geo-distributed deployments with variable latency, transient partitions, and high-latency wide-area links suffer under Distributed Erlang's full-mesh heartbeat approach. Partisan's TCP-based failure detection, topology specialization, and resilience to partitions make it ideal for multi-region deployments, edge computing scenarios, and hybrid cloud-edge architectures.

**High-throughput applications** with distinct traffic types gain immensely from channel separation. Systems processing large data volumes while maintaining coordination channels—distributed storage like Riak, real-time analytics platforms, message brokers—can isolate background maintenance traffic from latency-sensitive request paths. The Riak Core benchmark demonstrated this perfectly: anti-entropy transfers no longer block client requests when routed over separate channels, transforming system responsiveness.

**IoT and embedded systems** at scale find Partisan essential. Mobile applications with many ephemeral clients connecting to stable servers naturally fit client-server topology. Edge computing scenarios with sensors forming mesh networks suit HyParView's peer-to-peer approach. The LO/JACK LATAM deployment demonstrates this pattern—300,000 vehicles reporting to distributed server infrastructure requires topology flexibility impossible with Distributed Erlang. The Achlys framework targeting GRiSP embedded boards for precision agriculture similarly relies on Partisan's partition tolerance and eventual consistency for unreliable rural networks.

**CRDT-based eventually consistent systems** leverage Partisan's monotonic channels and HyParView's probabilistic membership. PlumDB using Epidemic Broadcast Trees over Partisan achieves efficient global replication without coordination overhead. The Lasp programming language—coordination-free distributed dataflow with CRDT storage—relies entirely on Partisan for message transport across 1,024+ nodes. These systems embrace weak consistency and partition tolerance, trading strong guarantees for availability and scale.

Conversely, Partisan is **not recommended** for small clusters under 50-60 nodes in single-datacenter deployments where Distributed Erlang performs adequately. Applications requiring strong consistency guarantees, stable membership, or synchronous coordination protocols like Raft or Paxos should carefully evaluate whether Partisan's probabilistic membership suits their needs. Systems heavily dependent on Distributed Erlang's `global` registry, extensive RPC usage, or transparent remote monitoring without modification effort may face migration challenges.

## Integration prospects: BEAM-native with conceptual portability

Partisan remains fundamentally a **BEAM-specific technology** tightly coupled to Erlang VM internals, process model, and external term format encoding. Full integration requires the BEAM runtime, making direct use impossible for non-BEAM languages. Both Erlang and Elixir enjoy first-class support—the latest v5.0.2 release provides comprehensive documentation for both ecosystems, with Elixir configuration examples and dependency management through Mix.

Other BEAM languages like Gleam can theoretically use Partisan through standard Erlang interop mechanisms, and maintainers explicitly mention Gleam in documentation roadmap plans. Any language compiling to BEAM bytecode can access Partisan via FFI patterns, though understanding Erlang behaviors and process model remains prerequisite. The barrier is conceptual familiarity rather than technical compatibility.

For **non-BEAM functional languages**, direct integration faces fundamental obstacles. Partisan replaces Distributed Erlang's distribution layer at a deep VM level, managing TCP connections for BEAM processes, handling BEAM-specific PIDs and references, and implementing BEAM OTP behaviors. The tight coupling to actor model assumptions—lightweight process isolation, supervision trees, message-passing semantics—makes extraction infeasible without rebuilding substantial BEAM infrastructure.

However, Partisan's **conceptual innovations** offer valuable lessons for other ecosystems. The core insights—multiple overlay topologies selectable at runtime, named channels with per-channel parallelism, separation of control and data planes, TCP-based implicit failure detection, bring-your-own overlay APIs—are fundamentally language-agnostic design patterns. Implementers in other ecosystems can draw inspiration without direct code reuse.

Practical integration approaches include **protocol-level bridges** where Partisan-based services expose standard interfaces (gRPC, REST, message queues) to non-BEAM languages. This hybrid architecture positions Partisan as distributed infrastructure layer while allowing polyglot application components. Organizations like Leapsight's Bondy API gateway demonstrate this pattern—Partisan handles distributed membership and messaging while exposing WAMP (Web Application Messaging Protocol) to any language supporting WebSocket.

Alternative functional language ecosystems could **reimagine Partisan's concepts** natively. Akka Cluster in Scala uses gossip-based membership but lacks Partisan's multi-topology approach and could benefit from channel concepts. Orleans in C#/.NET implements virtual actors with cluster membership but uses fixed topology; Partisan's ideas could improve geo-distribution capabilities. Haskell's Cloud Haskell provides distributed processes but suffers single-connection limitations parallel to Distributed Erlang's original problems. Each ecosystem could extract Partisan's key innovations—especially multiple connections, named channels, and runtime topology selection—without requiring BEAM compatibility.

## Current state: mature, actively maintained, production-ready

As of late 2025, Partisan stands as a mature, actively maintained project with stable APIs and proven production deployments. The **v5.0.2 stable release** (August 2025) represents the culmination of extensive refactoring completed in the v5.0.0 series. Maintenance transitioned from creator Christopher Meiklejohn to Alejandro Ramallo at Leapsight in June 2022, ensuring ongoing professional stewardship with commercial support availability.

The v5.0.0 overhaul brought **comprehensive API redesign** consolidating distribution primitives around `partisan` and `partisan_peer_service` modules that mirror Erlang's built-in equivalents. Complete OTP behavior implementations—`partisan_gen_server`, `partisan_gen_statem`, `partisan_gen_event`, plus supporting infrastructure—all pass adapted Erlang/OTP Common Test suites and satisfy Eqwalizer and Dialyzer type checking. This represents production-grade engineering with attention to correctness and type safety.

Performance improvements in v5.0.0 included replacing `orddict` with maps, implementing ETS-based connection caching for lower-latency lookups without contention, restructuring option computation for efficiency, and removing the `lager` dependency in favor of Erlang's native `logger`. Data structure updates like replacing `state_orset` CRDT with `state_awmap` address Kubernetes compatibility for handling IP address changes, while bug fixes throughout the leave operation, broadcast membership updates, and connection management during departures improve reliability.

**Production adoption** validates Partisan's readiness. LO/JACK LATAM's vehicle tracking platform has operated since 2019 at impressive scale—300,000 vehicles, 30+ million daily GPS transmissions—using Partisan for Erleans virtual actors and PlumDB storage. Bondy, the open-source application networking platform by Leapsight, implements WAMP over Partisan for microservices and IoT applications, with documented scale to 1,024 Erlang nodes. PlumDB, the globally-replicated eventually consistent database, builds on Partisan's Epidemic Broadcast Trees for efficient replication. Erleans, the Orleans-inspired virtual actor framework, uses Partisan as transport layer in production IoT systems.

**Community health** appears robust. The GitHub repository shows 994 stars and 65 forks with active issue tracking (24 open issues). Comprehensive documentation exists on HexDocs with API references, configuration guides, and a quick-reference cheatsheet. Active discussion occurs on Erlang Forums where maintainers participate and users report successful integration stories, particularly around digital twin technology and IoT platforms. The dedicated website partisan.dev serves as central information hub.

**Future roadmap** includes several enhancements. Per-channel parallelism settings will allow different connections per channel rather than global configuration. URI-encoded remote PIDs and references normalization will improve memory efficiency. Causal delivery features with disk-based storage for failover promise reliable causal broadcast implementations. Overlay tree construction improvements include implementing Thicket protocol for better epidemic broadcast performance and using per-channel `partisan_plumtree_broadcast` servers for increased throughput. OTP 25 and 26 compatibility maintenance ensures Partisan tracks latest Erlang releases, with QUIC transport exploration for performance gains beyond TCP.

Christopher Meiklejohn's foundational work established Partisan's academic credibility through papers at USENIX ATC 2019 and ApPLIED 2018, demonstrating scale to 1,024 nodes and performance improvements up to 18x under normal conditions, 30x with congestion. His transition to Microsoft Azure and work on resilient microservices, serverless systems, and fault injection testing leaves a strong legacy while Leapsight's commercial stewardship ensures continued development aligned with production needs.

The project trajectory suggests Partisan is positioned to become the standard distribution layer for large-scale BEAM applications, enabling application classes previously impossible in Erlang/Elixir—massive IoT platforms, geo-distributed systems, multi-region replication—while serving as reference implementation for distributed actor system innovations applicable beyond the BEAM ecosystem. For teams hitting Distributed Erlang's scalability walls or deploying in challenging network environments, Partisan represents not merely an alternative but the production-proven solution to long-standing distributed systems challenges in the actor model paradigm.
