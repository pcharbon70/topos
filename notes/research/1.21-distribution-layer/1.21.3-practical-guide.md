# Practical Partisan Integration Patterns for Topos

## Developer-Focused Implementation Guide

This document provides practical patterns for using Partisan within Topos, emphasizing pragmatic approaches that leverage category theory without requiring deep theoretical knowledge.

## Quick Start: Basic Partisan Usage in Topos

### Setting Up a Distributed Application

```topos
-- Simple distributed application setup
module MyDistributedApp where

import Topos.Partisan as P
import Topos.Effects

-- Configuration for your distributed system
config distributed_config : P.Config = {
  topology = P.Topology.FullMesh,     -- Start simple
  channels = 5,                        -- Parallel connections
  compression = true,                  -- Enable compression
  fault_detection = P.SwimDetection   -- Efficient failure detection
}

-- Main entry point
flow start_node : String -> () / {P.Distributed, IO} =
  node_name <- get_node_name()
  
  -- Initialize Partisan with config
  perform P.init(distributed_config)
  
  -- Join or create cluster
  match node_name {
    | "seed" -> 
        perform P.create_cluster("my_cluster")
        log("Created cluster as seed node")
        
    | _ ->
        perform P.join_cluster("seed@host", "my_cluster")
        log("Joined cluster")
  }
  
  -- Start application services
  start_services()
```

## Core Patterns

### Pattern 1: Reliable Request-Response

Traditional RPC often fails in distributed systems. Here's a robust alternative:

```topos
-- Reliable request-response with timeout and retry
module ReliableRPC where

type Request a = {
  id: UUID,
  payload: a,
  sender: Node,
  timestamp: Time
}

type Response b = {
  request_id: UUID,
  result: Result b Error,
  responder: Node
}

-- Effect for RPC operations
effect RPC {
  operation call(node: Node, req: Request): Response / {Timeout}
  operation serve(handler: Request -> Response): Unit
}

-- Smart handler with retries and fallback
handler resilient_rpc : Handler(RPC) / {P.Distributed} = {
  state pending: Map(UUID, Promise(Response)) = Map.empty
  state timeout_ms = 5000
  state max_retries = 3
  
  call(node, req) => {
    let promise = Promise.create()
    pending := pending.insert(req.id, promise)
    
    -- Try primary node with retries
    let result = retry_with_backoff(max_retries, fn(attempt) ->
      perform P.send(node, req)
      
      match Promise.wait_timeout(promise, timeout_ms) {
        | Ok(response) -> Ok(response)
        | Timeout -> 
            if attempt < max_retries {
              log(f"Timeout on attempt {attempt}, retrying...")
              Retry
            } else {
              -- Fallback to another node
              fallback_node <- find_alternative_node(node)
              perform P.send(fallback_node, req)
              Promise.wait_timeout(promise, timeout_ms)
            }
      }
    )
    
    pending := pending.delete(req.id)
    result
  }
  
  serve(handler) => {
    spawn_permanent {
      loop {
        match perform P.recv() {
          | Request(req) ->
              let response = handler(req)
              perform P.send(req.sender, response)
              
          | Response(resp) ->
              match pending[resp.request_id] {
                | Some(promise) -> Promise.fulfill(promise, resp)
                | None -> ()  -- Late response, ignore
              }
        }
      }
    }
  }
}

-- Usage example
flow fetch_user_data : UserId -> User / {RPC, P.Distributed} =
  user_service <- discover_service("user-service")
  
  request = {
    id = UUID.generate(),
    payload = GetUser(user_id),
    sender = self(),
    timestamp = now()
  }
  
  response <- perform RPC.call(user_service, request)
  
  match response.result {
    | Ok(user) -> user
    | Error(e) -> 
        log_error(f"Failed to fetch user: {e}")
        User.default()
  }
```

### Pattern 2: Distributed State Management

Using Partisan with CRDTs for eventually consistent state:

```topos
-- Distributed state with automatic synchronization
module DistributedState where

-- Type class for mergeable state
trait Mergeable a where
  merge : a -> a -> a
  
-- Distributed state container
type DState a : Mergeable a => {
  local: a,
  version: VectorClock,
  sync_strategy: SyncStrategy
}

type SyncStrategy = 
  | Eager         -- Sync on every change
  | Periodic(ms)  -- Sync every N milliseconds
  | Lazy          -- Sync only on read

-- Effect for distributed state operations
effect StateSync {
  operation get[a : Mergeable](): a
  operation update[a : Mergeable](f: a -> a): Unit
  operation sync[a : Mergeable](): Unit
}

-- Handler using Partisan for state distribution
handler partisan_state[a : Mergeable](initial: a, strategy: SyncStrategy) 
  : Handler(StateSync) / {P.Distributed} = {
  
  state dstate: DState a = {
    local = initial,
    version = VectorClock.empty,
    sync_strategy = strategy
  }
  
  init() => {
    match strategy {
      | Periodic(ms) ->
          schedule_repeated(ms, fn() -> perform sync())
      | _ -> ()
    }
    
    -- Listen for state updates from peers
    spawn_receiver {
      loop {
        match perform P.recv() {
          | StateUpdate(remote_state, remote_clock) ->
              if VectorClock.concurrent(dstate.version, remote_clock) {
                -- Concurrent updates, merge
                dstate := {
                  local = merge(dstate.local, remote_state),
                  version = VectorClock.merge(dstate.version, remote_clock)
                }
              } else if VectorClock.happens_before(dstate.version, remote_clock) {
                -- Remote is newer
                dstate := {dstate with 
                  local = remote_state,
                  version = remote_clock
                }
              }
              -- Otherwise, our state is newer, ignore
        }
      }
    }
  }
  
  get() => {
    match strategy {
      | Lazy -> perform sync()
      | _ -> ()
    }
    dstate.local
  }
  
  update(f) => {
    dstate := {dstate with 
      local = f(dstate.local),
      version = VectorClock.increment(dstate.version, self())
    }
    
    match strategy {
      | Eager -> perform sync()
      | _ -> ()
    }
  }
  
  sync() => {
    members <- perform P.members()
    perform P.broadcast(
      members,
      StateUpdate(dstate.local, dstate.version)
    )
  }
}

-- Example: Distributed shopping cart
type Cart = Map(ProductId, Quantity)

instance Mergeable Cart where
  merge c1 c2 = Map.merge_with(max, c1, c2)  -- Take max quantity

flow shopping_cart_example : () / {StateSync, P.Distributed} =
  with partisan_state(Map.empty, Eager) {
    -- Add items to cart
    perform StateSync.update(fn(cart) ->
      cart |> Map.insert(product_1, 2)
           |> Map.insert(product_2, 1)
    )
    
    -- Read current cart (synchronized)
    current_cart <- perform StateSync.get()
    
    checkout(current_cart)
  }
```

### Pattern 3: Dynamic Topology Adaptation

Automatically switch topologies based on system conditions:

```topos
-- Adaptive topology management
module AdaptiveTopology where

type SystemMetrics = {
  node_count: Int,
  message_rate: Float,
  average_latency: Duration,
  network_partition_detected: Bool
}

-- Rules for topology selection
flow select_optimal_topology : SystemMetrics -> P.Topology =
  match metrics {
    -- Small cluster with low latency: use full mesh
    | {node_count: n, average_latency: l} 
        when n < 20 && l < 10ms ->
        P.Topology.FullMesh
    
    -- Large cluster: use peer-to-peer
    | {node_count: n} when n > 100 ->
        P.Topology.PeerToPeer(hyparview_config)
    
    -- Network partition: switch to gossip
    | {network_partition_detected: true} ->
        P.Topology.Gossip(epidemic_config)
    
    -- Default: client-server for predictability
    | _ ->
        P.Topology.ClientServer(auto_select_servers)
  }

-- Topology manager that adapts to conditions
flow adaptive_topology_manager : () / {P.Distributed, Process} =
  spawn_supervisor {
    loop {
      sleep(30000)  -- Check every 30 seconds
      
      metrics <- collect_system_metrics()
      current <- perform P.current_topology()
      optimal = select_optimal_topology(metrics)
      
      if current != optimal {
        log(f"Switching topology from {current} to {optimal}")
        
        -- Prepare for transition
        perform P.pause_new_connections()
        
        -- Switch topology
        perform P.set_topology(optimal)
        
        -- Reconnect with new topology
        perform P.reconnect_all()
        
        log("Topology switch complete")
      }
    }
  }

-- Usage with automatic adaptation
flow start_adaptive_system : () / {P.Distributed, Process} =
  -- Start with simple topology
  perform P.init({topology = P.Topology.FullMesh})
  
  -- Launch adaptive manager
  adaptive_topology_manager()
  
  -- Your application continues normally
  -- Topology adapts automatically in background
  run_application()
```

### Pattern 4: Partition-Tolerant Services

Building services that continue operating during network partitions:

```topos
-- Partition-tolerant service pattern
module PartitionTolerant where

type PartitionStrategy = 
  | ContinueLocal      -- Each partition operates independently
  | ElectLeader        -- Only partition with leader operates
  | ReadOnlyMode       -- All partitions go read-only
  | MergeOnReconnect   -- Track changes and merge later

effect PartitionAware {
  operation on_partition_detected(): Unit
  operation on_partition_healed(other_partitions: List(Partition)): Unit
  operation is_partitioned(): Bool
}

handler partition_handler(strategy: PartitionStrategy) 
  : Handler(PartitionAware) / {P.Distributed, StateSync} = {
  
  state partitioned: Bool = false
  state partition_log: List(Operation) = []
  state read_only: Bool = false
  
  on_partition_detected() => {
    partitioned := true
    
    match strategy {
      | ContinueLocal ->
          log("Partition detected, continuing with local operations")
          partition_log := []  -- Start tracking changes
          
      | ElectLeader ->
          am_leader <- run_leader_election()
          if not am_leader {
            read_only := true
            log("Not leader in partition, entering read-only mode")
          }
          
      | ReadOnlyMode ->
          read_only := true
          log("Partition detected, entering read-only mode")
          
      | MergeOnReconnect ->
          partition_log := []
          log("Partition detected, logging operations for merge")
    }
  }
  
  on_partition_healed(other_partitions) => {
    partitioned := false
    read_only := false
    
    match strategy {
      | MergeOnReconnect ->
          -- Exchange and merge partition logs
          for partition in other_partitions {
            their_log <- perform P.call(partition.leader, GetPartitionLog)
            merged_state <- merge_partition_logs(partition_log, their_log)
            perform StateSync.update(fn(_) -> merged_state)
          }
          partition_log := []
          
      | _ ->
          -- Force full state sync
          perform StateSync.sync()
    }
    
    log("Partition healed, normal operations resumed")
  }
  
  is_partitioned() => partitioned
}

-- Example usage: Partition-aware counter service
flow resilient_counter_service : () / {PartitionAware, StateSync, P.Distributed} =
  with partition_handler(MergeOnReconnect) {
    with partisan_state(0, Periodic(1000)) {
      
      -- Monitor for partitions
      spawn_monitor {
        loop {
          if detect_partition() {
            perform PartitionAware.on_partition_detected()
          }
          sleep(5000)
        }
      }
      
      -- Service operations
      serve {
        | Increment(n) ->
            if perform PartitionAware.is_partitioned() {
              -- Log operation during partition
              log_operation(Increment(n))
            }
            perform StateSync.update(fn(count) -> count + n)
            
        | GetCount ->
            perform StateSync.get()
      }
    }
  }
```

### Pattern 5: Service Mesh Integration

Building a service mesh on top of Partisan:

```topos
-- Service mesh functionality
module ServiceMesh where

type ServiceDefinition = {
  name: ServiceName,
  version: Version,
  endpoints: List(Endpoint),
  health_check: HealthCheck,
  load_balancing: LoadBalancingStrategy,
  circuit_breaker: CircuitBreakerConfig
}

type LoadBalancingStrategy = 
  | RoundRobin
  | LeastConnections
  | Random
  | Weighted(Map(Node, Weight))
  | Consistent(HashFunction)

-- Service mesh handler
handler service_mesh : Handler(ServiceMesh) / {P.Distributed} = {
  state registry: Map(ServiceName, List(ServiceInstance)) = Map.empty
  state circuit_breakers: Map(ServiceName, CircuitBreaker) = Map.empty
  
  discover(service_name) => {
    match registry[service_name] {
      | None -> Error(ServiceNotFound)
      | Some(instances) ->
          -- Filter healthy instances
          healthy = instances |> List.filter(is_healthy)
          
          if healthy.is_empty() {
            Error(NoHealthyInstances)
          } else {
            -- Apply load balancing
            selected = apply_load_balancing(healthy)
            Ok(selected)
          }
    }
  }
  
  call_service(service_name, request) => {
    -- Check circuit breaker
    breaker <- get_or_create_breaker(service_name)
    
    if breaker.is_open() {
      return Error(CircuitOpen)
    }
    
    -- Discover and call
    match perform discover(service_name) {
      | Error(e) -> 
          breaker.record_failure()
          Error(e)
          
      | Ok(instance) ->
          -- Add tracing headers
          traced_request = add_tracing(request)
          
          -- Call with timeout
          match timeout(5000, perform P.call(instance.node, traced_request)) {
            | Ok(response) ->
                breaker.record_success()
                Ok(response)
                
            | Timeout ->
                breaker.record_failure()
                
                -- Try fallback
                match fallback_for(service_name) {
                  | Some(fallback) -> fallback(request)
                  | None -> Error(ServiceTimeout)
                }
          }
    }
  }
  
  -- Automatic service registration
  register(definition) => {
    let instance = {
      node = self(),
      definition = definition,
      started_at = now(),
      status = Healthy
    }
    
    registry := registry.update(definition.name, 
      fn(instances) -> instances.append(instance))
    
    -- Broadcast registration
    perform P.broadcast(ServiceRegistered(instance))
    
    -- Start health reporting
    spawn_health_reporter(definition)
  }
}

-- Circuit breaker implementation
type CircuitBreaker = {
  failures: Int,
  successes: Int,
  state: CircuitState,
  last_failure: Option(Time),
  config: CircuitBreakerConfig
}

type CircuitState = Closed | Open | HalfOpen

impl CircuitBreaker {
  is_open : () -> Bool = 
    match state {
      | Open -> 
          -- Check if we should try half-open
          match last_failure {
            | Some(time) when now() - time > config.reset_timeout ->
                state := HalfOpen
                false
            | _ -> true
          }
      | _ -> false
    }
  
  record_success : () -> Unit = 
    successes := successes + 1
    if state == HalfOpen && successes >= config.half_open_successes {
      state := Closed
      failures := 0
    }
  
  record_failure : () -> Unit = 
    failures := failures + 1
    last_failure := Some(now())
    
    if failures >= config.failure_threshold {
      state := Open
    }
}

-- Example: Using the service mesh
flow microservice_example : Request -> Response / {ServiceMesh, P.Distributed} =
  -- Register our service
  perform ServiceMesh.register({
    name = "user-service",
    version = "1.0.0",
    endpoints = [GetUser, UpdateUser, DeleteUser],
    health_check = http_health_check("/health"),
    load_balancing = RoundRobin,
    circuit_breaker = {
      failure_threshold = 5,
      reset_timeout = 30s,
      half_open_successes = 2
    }
  })
  
  -- Call another service with all mesh features
  -- (load balancing, circuit breaking, tracing, etc.)
  perform ServiceMesh.call_service("order-service", GetOrders(user_id))
```

## Testing Patterns

### Testing Distributed Code with Partisan

```topos
-- Test utilities for Partisan
module PartisanTest where

-- Create isolated test cluster
flow test_cluster : Int -> TestCluster / {P.Distributed, Test} =
  nodes <- create_virtual_nodes(n)
  
  -- Use special test topology with controlled message delivery
  perform P.init({
    topology = P.Topology.Test(controlled_delivery),
    fault_injection = enabled
  })
  
  TestCluster {
    nodes = nodes,
    control = test_controller()
  }

-- Property-based testing for distributed algorithms
flow test_distributed_consensus : Property / {Test, P.Distributed} =
  property "consensus reached under partition" {
    forall (partition_scenario : PartitionScenario) {
      cluster <- test_cluster(5)
      
      -- Initialize consensus algorithm
      for node in cluster.nodes {
        on_node(node, init_consensus)
      }
      
      -- Inject partition
      cluster.control.inject_partition(partition_scenario)
      
      -- Propose values
      proposals <- generate_proposals()
      for (node, value) in zip(cluster.nodes, proposals) {
        on_node(node, propose(value))
      }
      
      -- Wait for consensus (with timeout)
      wait_until_timeout(10s, fn() ->
        decisions <- collect_decisions(cluster.nodes)
        all_equal(decisions)
      )
      
      -- Heal partition
      cluster.control.heal_partition()
      
      -- Verify eventual consistency
      eventually {
        decisions <- collect_decisions(cluster.nodes)
        assert(all_equal(decisions))
      }
    }
  }

-- Deterministic testing with controlled message ordering
flow test_message_ordering : () / {Test, P.Distributed} =
  test "causal delivery preserves order" {
    cluster <- test_cluster(3)
    
    -- Control message delivery order
    cluster.control.pause_delivery()
    
    on_node(cluster.nodes[0], fn() ->
      perform P.send(cluster.nodes[1], Msg1)
      perform P.send(cluster.nodes[2], Msg2)
    )
    
    on_node(cluster.nodes[1], fn() ->
      perform P.send(cluster.nodes[2], Msg3)
    )
    
    -- Deliver in specific order to test causality
    cluster.control.deliver_message(Msg1)
    cluster.control.deliver_message(Msg3)
    cluster.control.deliver_message(Msg2)
    
    -- Verify causal order maintained
    received <- on_node(cluster.nodes[2], get_received_messages)
    assert(causally_ordered(received))
  }
```

## Performance Optimization Patterns

### Channel Pooling for High Throughput

```topos
module ChannelPool where

-- Pool of channels with automatic sizing
type ChannelPool = {
  channels: Array(P.Channel),
  stats: ChannelStats,
  sizing_strategy: SizingStrategy
}

flow create_adaptive_pool : Node -> ChannelPool / {P.Distributed} =
  initial_size = estimate_initial_size(node)
  channels <- perform P.create_channels(node, initial_size)
  
  pool = {
    channels = channels,
    stats = ChannelStats.empty,
    sizing_strategy = Adaptive
  }
  
  -- Monitor and resize pool
  spawn_monitor {
    loop {
      sleep(1000)
      stats <- collect_channel_stats(pool)
      
      if stats.average_queue_length > threshold {
        -- Add more channels
        new_channel <- perform P.create_channel(node)
        pool.channels.append(new_channel)
        
      } else if stats.idle_ratio > 0.5 {
        -- Remove idle channels
        if pool.channels.length > 1 {
          idle_channel <- find_most_idle(pool.channels)
          perform P.close_channel(idle_channel)
          pool.channels.remove(idle_channel)
        }
      }
    }
  }
  
  pool

-- Smart channel selection
flow send_with_pool : ChannelPool -> Message -> () / {P.Distributed} =
  -- Select channel with least pending messages
  channel = pool.channels 
    |> Array.min_by(channel_queue_length)
    
  perform P.send_on_channel(channel, msg)
  pool.stats.record_send(channel)
```

## Migration Guide from Distributed Erlang

### Replacing Distributed Erlang with Partisan

```topos
-- Before (Distributed Erlang style)
flow old_style : () / {DistErlang} =
  -- Limited to ~50 nodes
  nodes <- erlang.nodes()
  
  -- Single connection bottleneck
  erlang.send(node, {name, message})
  
  -- Basic monitoring
  erlang.monitor_node(node, true)

-- After (Partisan style)
flow new_style : () / {P.Distributed} =
  -- Scales to 1000+ nodes
  nodes <- perform P.members()
  
  -- Parallel channels
  perform P.send(node, message)
  -- Or with specific channel
  perform P.send_on_channel(channel_3, node, message)
  
  -- Advanced failure detection
  perform P.monitor_with_strategy(node, SwimDetection)
```

## Best Practices

1. **Start Simple**: Begin with full mesh topology for development, optimize later
2. **Monitor Everything**: Use Partisan's built-in metrics and tracing
3. **Design for Partition**: Always consider split-brain scenarios
4. **Test with Chaos**: Use fault injection to test resilience
5. **Profile Channel Usage**: Optimize channel count based on actual traffic
6. **Use Type-Safe Messages**: Leverage Topos's type system for message safety
7. **Implement Idempotency**: Design operations to be safely retryable
8. **Version Your Protocols**: Plan for rolling upgrades

## Common Pitfalls and Solutions

| Pitfall | Solution |
|---------|----------|
| Over-provisioning channels | Use adaptive channel pooling |
| Ignoring network partitions | Implement partition detection and healing strategies |
| Assuming message ordering | Use causal delivery when order matters |
| Blocking on remote calls | Always use timeouts and fallbacks |
| Static topology selection | Implement adaptive topology switching |
| Not handling node failures | Use supervision and automatic reconnection |

## Conclusion

Partisan integration in Topos provides a powerful, flexible foundation for distributed systems that scales far beyond traditional Distributed Erlang while maintaining type safety and compositional reasoning through category theory. The patterns shown here demonstrate that complex distributed behaviors can be expressed elegantly and safely using Topos's effect system combined with Partisan's proven runtime.
