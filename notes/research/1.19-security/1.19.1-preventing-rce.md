**Language-Level Security Techniques for RCE Prevention**

**Practical Type-Based Taint Checking (Karimipour _et al._, ECOOP 2025)**

A recent ECOOP paper introduces **TaintTyper**, a modular type-based taint analysis built into the compiler[arxiv.org](https://arxiv.org/html/2504.18529v1#:~:text=First%2C%20we%20present%20a%20new,9X%20faster%20checking%20time). TaintTyper uses _type qualifiers_ (e.g. tainted&lt;T&gt;) to mark untrusted input and statically enforce that taint cannot flow into sensitive operations without sanitization. It automatically infers missing taint annotations (even for generic types) and handles libraries pragmatically to minimize false positives[arxiv.org](https://arxiv.org/html/2504.18529v1#:~:text=First%2C%20we%20present%20a%20new,9X%20faster%20checking%20time). In benchmarks it matched or exceeded the recall of whole-program analyzers while running orders of magnitude faster[arxiv.org](https://arxiv.org/html/2504.18529v1#:~:text=First%2C%20we%20present%20a%20new,9X%20faster%20checking%20time). **Practical insight:** This suggests integrating a qualifier-based type system into the new language: by defining tainted vs. untainted types and requiring explicit checks (or sanitizers) to convert them, many injection and data-flow vulnerabilities can be caught at compile time[arxiv.org](https://arxiv.org/html/2504.18529v1#:~:text=First%2C%20we%20present%20a%20new,9X%20faster%20checking%20time).

**Capability and Linear Types (Austral blog, 2023)**

Austral is a modern language designed around strict static types, **linear types**, and **capability-based security**[borretti.me](https://borretti.me/article/introducing-austral#:~:text=Austral%20is%20a%20new%20systems,based%20security%2C%20and%20strong%20modularity)[borretti.me](https://borretti.me/article/introducing-austral#:~:text=3.%20Capability,secure%20authorization%20tokens%20for%20code). Linear types enforce _use-once_ resource protocols (e.g. open/write/close for a file handle) at compile time, guaranteeing no use-after-close or leaks without runtime overhead[borretti.me](https://borretti.me/article/introducing-austral#:~:text=you%20need%20a%20PhD%20in,category%20theory%20to%20understand). The figure below illustrates a file's open/write/close lifecycle as a graph of allowed transitions (nodes are calls) - illegal paths (omitted here) are statically forbidden by Austral's type rules:

_Austral enforces correct resource usage via linear types. The figure (from \[17\]) shows legal call transitions: e.g. an opened file can be written or closed. By design, sequences like "write after close" cannot compile_[_borretti.me_](https://borretti.me/article/introducing-austral#:~:text=you%20need%20a%20PhD%20in,category%20theory%20to%20understand)_._

Austral also treats I/O and other privileges as _capabilities_: code must explicitly hold a "network" or "filesystem" capability to perform that action[borretti.me](https://borretti.me/article/introducing-austral#:~:text=3.%20Capability,secure%20authorization%20tokens%20for%20code). For example, a string-padding library without network permission can never send data externally, preventing "left-pad" style backdoors[borretti.me](https://borretti.me/article/introducing-austral#:~:text=3.%20Capability,secure%20authorization%20tokens%20for%20code). **Practical insight:** Adopting linear/affine types can enforce resource lifecycles (preventing memory corruption and leaks), and embedding capabilities into function signatures can ensure code can't access the network, file system, etc., unless explicitly permitted[borretti.me](https://borretti.me/article/introducing-austral#:~:text=you%20need%20a%20PhD%20in,category%20theory%20to%20understand)[borretti.me](https://borretti.me/article/introducing-austral#:~:text=3.%20Capability,secure%20authorization%20tokens%20for%20code). These design-by-default features catch many security mistakes at compile time.

**Strict Type Safety (GHC Safe Haskell, 2025)**

Haskell's **Safe Haskell** extension enforces strict compile-time safety by disallowing unsafe operations (like unsafePerformIO) and demanding that only trusted modules expose IO or unsafe features[ghc.gitlab.haskell.org](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/safe_haskell.html#:~:text=Safe%20Haskell%20is%20an%20extension,the%20types%20of%20programs%20trustable). It effectively partitions code into "safe" and "trusted" sets: untrusted code cannot use unsafe primitives or perform arbitrary IO, making its types fully reliable[ghc.gitlab.haskell.org](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/safe_haskell.html#:~:text=Safe%20Haskell%20is%20an%20extension,the%20types%20of%20programs%20trustable). This stricter type discipline lets libraries implement secure abstractions (like information-flow controls or capability-based APIs) on top of the language, because they can assume purity of "safe" code[ghc.gitlab.haskell.org](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/safe_haskell.html#:~:text=Systems%20such%20as%20information%20flow,to%20build%20such%20secure%20systems). **Practical insight:** A similar approach in the new language would be to provide a strict mode or dialect that forbids dynamic or unsafe features by default, so that any code can rely on type-based guarantees. By restricting or annotating side effects (e.g. requiring effect/type annotations for IO), the language ensures at compile time that untrusted code cannot subvert abstractions[ghc.gitlab.haskell.org](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/safe_haskell.html#:~:text=Systems%20such%20as%20information%20flow,to%20build%20such%20secure%20systems).

**BEAM-Specific Guidance: Sandboxing Untrusted Code (Erlef Security WG)**

Official Erlang/Elixir security guidelines note that **BEAM lacks built-in access control between processes**[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/sandboxing.html#:~:text=The%20BEAM%20runtime%20has%20very,the%20other%20nodes%20as%20well). In practice, this means any code running on the VM can fully access VM interfaces and even other nodes, so there is _no_ safe "in-VM" sandbox. The guidance explicitly warns: **"Do not use file:script/1,2 or file:eval/1,2 on untrusted input, or in production"**[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/sandboxing.html#:~:text=,in%20production%20code%20at%20all) (and similarly forbid Code.eval_string in Elixir). In other words, dynamic code evaluation must be avoided for user-supplied code. **Practical insight:** For an experimental BEAM language, one should disallow or heavily restrict any form of eval or dynamic compilation on untrusted input. Instead, untrusted logic should run in a separate, sandboxed environment (for example using an embedded interpreter with its own boundary) rather than directly in the BEAM VM[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/sandboxing.html#:~:text=,in%20production%20code%20at%20all).

**BEAM-Specific Guidance: Safe Deserialization (Erlef Security WG)**

The Erlef security docs also warn that **Erlang's external term format (ETF) deserialization can invoke arbitrary code** if not handled carefully[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/serialisation.html#:~:text=When%20deserializing%20External%20Term%20Format,a%20text%20file%20using%20file%3Aconsult%2F1). In particular, deserializing a function (closure) from binary data can cause that function to execute (for example via Elixir's Enum protocol) - a known RCE vector[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/serialisation.html#:~:text=When%20deserializing%20External%20Term%20Format,a%20text%20file%20using%20file%3Aconsult%2F1)[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/serialisation.html#:~:text=,Base.encode64). The recommendations are to _never deserialize functions or arbitrary terms from untrusted input_, and to always use safe modes. For instance, one should call :erlang.binary_to_term(binary, \[:safe\]) and/or libraries like Plug.Crypto.non_executable_binary_to_term/2, which reject any function or exotic term[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/serialisation.html#:~:text=When%20deserializing%20External%20Term%20Format,a%20text%20file%20using%20file%3Aconsult%2F1). **Practical insight:** The language runtime should avoid offering raw term deserialization on user data, or at least require a "safe" flag that forbids functions and limits atom creation. Instead, use well-understood safe formats (JSON, protocol buffers, etc.) or validated schemas for external input. This prevents attacker-controlled data from causing code execution or resource exhaustion[security.erlef.org](https://security.erlef.org/secure_coding_and_deployment_hardening/serialisation.html#:~:text=When%20deserializing%20External%20Term%20Format,a%20text%20file%20using%20file%3Aconsult%2F1).

**Memory Safety for RCE Prevention (ACM Queue, 2023)**

Industry reports emphasize that eliminating memory-unsafe bugs is a cornerstone of RCE defense. Companies that rewrote core components in Rust (memory-safe by default) saw on the order of **70% fewer memory-safety bugs**[queue.acm.org](https://queue.acm.org/detail.cfm?id=3773095#:~:text=been%20written%20in%20C%20or,groups%20began%20to%20take%20notice). This is no accident: classic RCE exploits (buffer overflows, use-after-free) arise from memory errors[queue.acm.org](https://queue.acm.org/detail.cfm?id=3773095#:~:text=Memory,into%20a%20botnet%2C%20and%20more). As the ACM Queue article notes, "Memory-safety issues are often a central building block" for remote code execution[queue.acm.org](https://queue.acm.org/detail.cfm?id=3773095#:~:text=Memory,into%20a%20botnet%2C%20and%20more). **Practical insight:** Ensuring memory safety by design (through garbage collection, linear/ownership types, bounds checks, etc.) is crucial. For example, using a traced GC (like BEAM does) or ownership/linear types (like Rust or Austral) will natively eliminate many RCE vectors. It is far easier to enforce memory safety in the language type system than to patch each vulnerability later[queue.acm.org](https://queue.acm.org/detail.cfm?id=3773095#:~:text=been%20written%20in%20C%20or,groups%20began%20to%20take%20notice)[queue.acm.org](https://queue.acm.org/detail.cfm?id=3773095#:~:text=Memory,into%20a%20botnet%2C%20and%20more).

**WebAssembly Sandboxing (Research 2025)**

WebAssembly (Wasm) provides a modern, **secure sandbox** for running untrusted code. Studies note that _"Wasm modules operate in a secure, sandboxed environment, making them ideal for executing untrusted code."_ The Wasm memory model forbids raw pointers and shares no memory with the host, so traditional memory corruption or arbitrary jumps are impossible[atlarge-research.com](https://atlarge-research.com/pdfs/mjansen-wasm-2025.pdf#:~:text=WebAssembly%20%28Wasm%29%20is%20a%20stack,native%20performance%20%5B18%5D.%20The). Wasm is also lightweight and near-native-fast. **Practical insight:** One can embed a Wasm VM to isolate plugins or user scripts. By compiling user code to Wasm and executing it in a sandbox (with controlled imports), the language gains strong protection: any misbehaving module is confined. This is a proven approach in many modern systems for running third-party code safely[atlarge-research.com](https://atlarge-research.com/pdfs/mjansen-wasm-2025.pdf#:~:text=WebAssembly%20%28Wasm%29%20is%20a%20stack,native%20performance%20%5B18%5D.%20The).

**BEAM Process Isolation (Code BEAM Lite Stockholm, 2025)**

The BEAM VM itself is designed for **process-level isolation**. Each Erlang/Elixir process has its own heap and scheduler, so a crash or memory fault in one process won't corrupt another[codebeamstockholm.com](https://codebeamstockholm.com/keynotes/security-under-the-beam/#:~:text=The%20BEAM%20VM%20is%20a,OS%20up%20through%20the%20BEAM). The keynote "Security Under the BEAM" highlights that _"isolated code recovers from faults"_ and _"isolated data means garbage collection is fast and simple"_[codebeamstockholm.com](https://codebeamstockholm.com/keynotes/security-under-the-beam/#:~:text=The%20BEAM%20VM%20is%20a,OS%20up%20through%20the%20BEAM). **Practical insight:** Even though BEAM lacks fine-grained permissions, one can use its actor model for coarse isolation: run untrusted code in a separate process or node with limited linking, so that if it panics or exhausts memory it cannot kill the whole VM. In other words, leverage lightweight process isolation: supervise user code separately and never link it into core processes. This ensures that even if an exploit occurs, its effects are contained to a minimal part of the system[codebeamstockholm.com](https://codebeamstockholm.com/keynotes/security-under-the-beam/#:~:text=The%20BEAM%20VM%20is%20a,OS%20up%20through%20the%20BEAM).
