# Topos Networking: Category-Theoretic Communication Patterns for BEAM

Topos networking provides a comprehensive, category-theory-grounded approach to network communication on the BEAM virtual machine. This design respects categorical principles while embracing BEAM's actor model, delivering both elegant abstractions and practical performance.

## Core Philosophy: Effects, Streaming, and Composition

The networking layer is built on three foundational categorical concepts. **Functors transform network data** while preserving structure through the `map` operation on requests, responses, and streams. **Monads sequence effectful operations**, with the `IO` effect capturing all network side effects, `Result` managing success and failure paths, and `Validated` accumulating multiple validation errors. **Arrows model bidirectional protocols**, representing request/response cycles as morphisms in the Kleisli category where middleware composes naturally through categorical composition laws.

This categorical foundation ensures that networking code is not just correct but *provably* composable. When you compose two middleware functions, you get another middleware function. When you transform a stream, the transformation preserves the streaming properties. The mathematics guarantees it works.

## The Type-Level Architecture

### Network Effect Hierarchy

The effect system distinguishes between pure transformations and effectful operations through explicit type-level tracking:

```topos
-- Base effect for all I/O operations
effect IO[A] {
  pure: A -> IO[A]
  bind: IO[A] -> (A -> IO[B]) -> IO[B]
}

-- Network-specific effect capturing failures
effect Net[A] = IO[Result[NetError, A]] {
  -- Result is Either-like: Ok(value) | Err(error)
  -- Automatically tracks potential network failures
}

-- Stream effect for chunked data
effect Stream[F[_], A] where F: Monad {
  -- F is the underlying effect (usually IO or Net)
  -- A is the element type
  -- Chunks flow through with backpressure
}
```

The `Net` effect is a monad transformer, stacking `Result` over `IO`. This composition means network operations can fail (captured by `Result`) and perform I/O (captured by `IO`), with both effects tracked in the type system. Every network function's type signature explicitly declares what effects it performs, making side effects visible and controlled.

### Core Network Types

Request and response types are immutable record types with lens-based accessors:

```topos
type Method = Get | Post | Put | Patch | Delete | Head | Options | Custom(String)

type Headers = Map[String, List[String]]

type Request = {
  method: Method,
  url: URI,
  headers: Headers,
  body: Body,
  options: RequestOptions,
  private: Map[Symbol, Any]  -- metadata for middleware
}

type Response = {
  status: Int,
  headers: Headers,
  body: Body,
  request: Request  -- reference to originating request
}

type Body = 
  | Empty
  | Bytes(ByteString)
  | Text(String)
  | Json(JsonValue)
  | Form(Map[String, String])
  | Stream(Stream[IO, Chunk[Byte]])
  
type Chunk[A] = {
  data: List[A],
  size: Int
}
```

The `private` field enables middleware to store metadata without polluting the public API. Middleware can tag requests with tracing IDs, authentication contexts, or custom state using Symbol keys that won't conflict across different middleware implementations.

## Transport Layer: Finch-Inspired Connection Management

### Pool Architecture with Protocol Awareness

The transport layer manages connection pools with distinct strategies for HTTP/1.1 and HTTP/2, mirroring Finch's intelligent approach:

```topos
type PoolStrategy = 
  | Http1Pool { size: Int, count: Int }  -- Multiple connections, no multiplexing
  | Http2Pool { count: Int }              -- Multiplexed connections
  | AdaptivePool                          -- Protocol negotiation via ALPN

type PoolConfig = {
  strategy: PoolStrategy,
  conn_max_idle_time: Duration | Infinity,
  pool_max_idle_time: Duration | Infinity,
  conn_opts: ConnectionOptions
}

type ConnectionOptions = {
  timeout: Duration,
  protocols: List[Protocol],  -- [:http1], [:http2], or both
  transport_opts: TransportOptions,
  proxy: Option[ProxyConfig]
}

-- Configure pools per host
module Transport {
  -- Create transport with pool configuration
  let new: Map[HostKey, PoolConfig] -> IO[Transport] = ...
  
  -- HostKey identifies pool: (Scheme, Host, Port)
  type HostKey = (Scheme, String, Int)
  
  -- Example configuration
  let example_config = Map.of([
    (Default, PoolConfig {
      strategy = Http1Pool { size = 50, count = 1 },
      conn_max_idle_time = Duration.minutes(5),
      ...
    }),
    ((Https, "api.example.com", 443), PoolConfig {
      strategy = Http2Pool { count = 4 },
      ...
    })
  ])
}
```

**HTTP/1.1 pools** use multiple connections per pool because HTTP/1.1 requires one request per connection at a time. The `size` parameter controls connections per pool, while `count` creates multiple pool instances. More pools distribute load but may reduce connection reuse—a tradeoff that developers control explicitly.

**HTTP/2 pools** leverage multiplexing, so each connection handles many concurrent requests. The pool size is always 1, and `count` increases capacity by adding more multiplexed connections. This design respects the fundamental differences between protocols while presenting a unified interface.

### Connection Lifecycle as Coalgebra

Stateful protocols are modeled coalgebraically, where state transitions map current state to possible next states:

```topos
-- Coalgebra: State -> F[State]
type ConnectionState[A] = 
  | Idle
  | Connecting
  | Connected(Connection)
  | Closed(Reason)
  | Failed(Error)

-- State transition functor
type ConnectionF[S] =
  | StayIdle(S)
  | StartConnect(Host, Port, S)
  | HandleData(ByteString, S)
  | HandleClose(Reason, S)
  | HandleError(Error, S)

-- Coalgebra for connection lifecycle
let transition: ConnectionState[A] -> IO[ConnectionF[ConnectionState[A]]] = {
  case Idle -> 
    IO.pure(StayIdle(Idle))
  
  case Connecting ->
    IO.async_connect()
      .map(conn -> HandleData(ByteString.empty, Connected(conn)))
      .catch(err -> HandleError(err, Failed(err)))
  
  case Connected(conn) ->
    conn.read()
      .map(data -> HandleData(data, Connected(conn)))
      .catch_close(reason -> HandleClose(reason, Closed(reason)))
  
  case Closed(_) | Failed(_) ->
    IO.pure(StayIdle(Idle))
}
```

This coalgebraic model captures the infinite/ongoing nature of network connections. Unlike algebraic data types that model finite structures, coalgebras model potentially infinite behaviors. Each state transition produces the next state wrapped in an effect, allowing the state machine to run indefinitely until an explicit termination condition.

### Resource Safety with Bracket Pattern

BEAM's process supervision ensures cleanup, but Topos adds explicit resource bracketing:

```topos
-- Bracket ensures cleanup regardless of success/failure
let with_connection: PoolConfig -> (Connection -> Net[A]) -> Net[A] =
  |config, use_conn|
    bracket(
      acquire = Transport.checkout_connection(config),
      release = |conn| Transport.checkin_connection(conn),
      use = use_conn
    )

-- Usage
let fetch_data: URI -> Net[Response] = |uri|
  with_connection(pool_config, |conn| {
    let request = Request.new(Get, uri)
    conn.send(request)
      .bind(|_| conn.receive())
  })
```

The `bracket` combinator guarantees that `release` runs even if `use` throws an exception or the process crashes. This combines BEAM's supervision trees with functional resource management, giving both fault tolerance and deterministic cleanup.

## High-Level HTTP Client: Req-Inspired Composability

### Request Builder as Free Monad

The request builder uses a free monad to separate description from interpretation, enabling testing and multiple execution strategies:

```topos
-- DSL operations as a functor
type RequestOp[Next] =
  | SetMethod(Method, Next)
  | SetURL(URI, Next)
  | AddHeader(String, String, Next)
  | SetBody(Body, Next)
  | AddPlugin(Plugin, Next)

-- Free monad over RequestOp
type RequestBuilder[A] = Free[RequestOp, A]

-- Smart constructors
module RequestBuilder {
  let method: Method -> RequestBuilder[Unit] = 
    |m| liftF(SetMethod(m, Unit))
  
  let url: String -> RequestBuilder[Unit] =
    |u| liftF(SetURL(URI.parse(u), Unit))
  
  let header: String -> String -> RequestBuilder[Unit] =
    |name, value| liftF(AddHeader(name, value, Unit))
  
  let json: JsonValue -> RequestBuilder[Unit] =
    |data| liftF(SetBody(Body.Json(data), Unit))
  
  -- Monadic composition via do-notation
  let build_request = do {
    method(Post)
    url("https://api.example.com/users")
    header("Authorization", "Bearer token")
    json(Json.object([("name", "Alice")]))
  }
}
```

The free monad pattern separates *what* to do (the DSL operations) from *how* to do it (the interpreter). This enables multiple interpreters: one for production that makes real requests, one for testing that returns mocked responses, and one for logging that records operations without executing them.

### Step-Based Middleware Pipeline

Request processing flows through a three-stage pipeline inspired by Req, where each stage is a list of composable functions:

```topos
-- Three types of steps, each with specific type signature
type RequestStep = Request -> Net[Request | (Request, Response)]
type ResponseStep = (Request, Response) -> Net[(Request, Response)]
type ErrorStep = (Request, NetError) -> Net[(Request, Response) | (Request, NetError)]

-- Pipeline configuration
type Pipeline = {
  request_steps: List[(Symbol, RequestStep)],
  response_steps: List[(Symbol, ResponseStep)],
  error_steps: List[(Symbol, ErrorStep)]
}

-- Example pipeline
let default_pipeline = Pipeline {
  request_steps = [
    (:put_user_agent, Steps.put_user_agent),
    (:compress, Steps.request_compression),
    (:auth, Steps.authenticate),
    (:encode_body, Steps.encode_json)
  ],
  response_steps = [
    (:decompress, Steps.decompress_body),
    (:decode, Steps.decode_json),
    (:validate, Steps.validate_response)
  ],
  error_steps = [
    (:retry, Steps.retry_transient),
    (:log, Steps.log_error)
  ]
}
```

Each step can transform its input, short-circuit to the next stage, or halt the pipeline entirely. Request steps can return `(Request, Response)` to skip remaining request steps and jump directly to response processing. Response steps can return `(Request, NetError)` to jump to error handling. Error steps can return `(Request, Response)` to recover and jump back to response processing.

This design makes the pipeline bidirectional: errors can recover to responses, enabling retry logic to work naturally. The categorical foundation ensures that composing steps preserves the pipeline properties.

### Steps as Natural Transformations

Middleware steps are natural transformations between functors in the Kleisli category:

```topos
-- Kleisli arrow: A -> F[B] where F is a monad
type Kleisli[F[_], A, B] = A -> F[B] where F: Monad

-- HTTP service is a Kleisli arrow
type HttpService[F[_]] = Kleisli[F, Request, Response]

-- Middleware transforms services
type Middleware[F[_]] = HttpService[F] -> HttpService[F]

-- Middleware composes categorically
let compose_middleware: Middleware[F] -> Middleware[F] -> Middleware[F] =
  |m1, m2, service| m1(m2(service))

-- Example: logging middleware
let logging: Middleware[Net] = |service, request| {
  do {
    let! _ = IO.println("Request: " ++ Request.to_string(request))
    let! response = service(request)
    let! _ = IO.println("Response: " ++ Int.to_string(response.status))
    return response
  }
}

-- Example: authentication middleware
let auth: String -> Middleware[Net] = |token, service, request| {
  let authenticated = Request.add_header(request, "Authorization", "Bearer " ++ token)
  service(authenticated)
}

-- Compose middleware
let app = logging >> auth(token) >> service
```

The composition operator `>>` is Kleisli composition, which is associative by the monad laws. This means `(m1 >> m2) >> m3 = m1 >> (m2 >> m3)`, so middleware order is well-defined and predictable.

### Plugin Architecture

Plugins are modules that attach steps to the pipeline, using the free monad pattern for configuration:

```topos
-- Plugin trait
trait Plugin {
  let attach: Pipeline -> Pipeline
  let options: Map[Symbol, Any]
}

-- Example plugin: rate limiting
module RateLimitPlugin: Plugin {
  type State = {
    requests: Map[String, (Int, Instant)],  -- host -> (count, window_start)
    limit: Int,
    window: Duration
  }
  
  let new: Int -> Duration -> Plugin = |limit, window| {
    let state = State { 
      requests = Map.empty(), 
      limit = limit, 
      window = window 
    }
    
    Plugin {
      attach = |pipeline| {
        let check_rate = |request| {
          do {
            let host = request.url.host
            let! current = get_count(state, host)
            if current >= state.limit then
              Net.error(RateLimitError(host))
            else {
              let! _ = increment_count(state, host)
              Net.pure(request)
            }
          }
        }
        
        Pipeline.prepend_request_step(pipeline, :rate_limit, check_rate)
      },
      options = Map.of([(:limit, limit), (:window, window)])
    }
  }
}

-- Usage
let client = 
  Http.new()
  |> attach_plugin(RateLimitPlugin.new(100, Duration.seconds(60)))
  |> attach_plugin(CachingPlugin.new())
  |> attach_plugin(MetricsPlugin.new())
```

Plugins compose through pipeline transformation. Each plugin's `attach` function receives the current pipeline and returns a modified pipeline. Multiple plugins attach sequentially, building up a rich pipeline from simple, focused components.

## Streaming with Backpressure

### Stream as a Functor

Streams are functors over chunks, supporting all standard functor operations:

```topos
-- Stream type with effect tracking
type Stream[F[_], A] where F: Monad = {
  -- Internal representation as pull-based stream
  step: StreamState -> F[(StreamState, Option[Chunk[A]])]
}

-- Stream is a functor
impl Functor[Stream[F, *]] where F: Monad {
  let map: (A -> B) -> Stream[F, A] -> Stream[F, B] = 
    |f, stream| {
      Stream {
        step = |state| {
          stream.step(state).map(|(next_state, maybe_chunk)| {
            let mapped_chunk = maybe_chunk.map(|chunk| {
              Chunk { 
                data = chunk.data.map(f), 
                size = chunk.size 
              }
            })
            (next_state, mapped_chunk)
          })
        }
      }
    }
}

-- Stream is also a monad
impl Monad[Stream[F, *]] where F: Monad {
  let pure: A -> Stream[F, A] = |a| {
    Stream { step = |state| F.pure((state, Some(Chunk.singleton(a)))) }
  }
  
  let bind: Stream[F, A] -> (A -> Stream[F, B]) -> Stream[F, B] =
    |stream, f| flatten(stream.map(f))
}
```

The functor instance enables transforming stream elements without materializing the entire stream. The `map` operation fuses automatically—multiple consecutive `map` calls become a single transformation pass.

### Pull-Based Backpressure Model

Backpressure is inherent in the pull model, where consumers request chunks only when ready:

```topos
-- Consumer pulls chunks from stream
type Consumer[F[_], A, B] where F: Monad = {
  init: B,
  consume: B -> Chunk[A] -> F[(B, ConsumerAction)]
}

type ConsumerAction = 
  | Continue    -- Request more chunks
  | Halt        -- Stop consuming

-- Stream execution with consumer control
let run_stream: Stream[F, A] -> Consumer[F, A, B] -> F[B] =
  |stream, consumer| {
    let rec loop = |state, acc| {
      do {
        let! (next_state, maybe_chunk) = stream.step(state)
        match maybe_chunk {
          case None -> F.pure(acc)
          case Some(chunk) -> {
            let! (next_acc, action) = consumer.consume(acc, chunk)
            match action {
              case Continue -> loop(next_state, next_acc)
              case Halt -> F.pure(next_acc)
            }
          }
        }
      }
    }
    loop(StreamState.init, consumer.init)
  }
```

The consumer explicitly signals when to continue or halt. The producer only generates the next chunk when `loop` calls `stream.step`, creating natural backpressure. The consumer's processing speed determines the production rate.

### Chunk-Based Processing

Chunking batches elements for efficiency while maintaining composability:

```topos
-- HTTP response streaming with chunks
let stream_response: Request -> Net[Stream[IO, Chunk[Byte]]] = |request| {
  do {
    let! conn = Transport.checkout()
    let! _ = conn.send(request)
    
    -- Create stream that pulls chunks from connection
    let stream = Stream {
      step = |state| {
        conn.read_chunk(state.buffer_size)
          .map(|bytes| {
            if ByteString.is_empty(bytes) then
              (state, None)  -- End of stream
            else
              (state, Some(Chunk { data = bytes, size = ByteString.length(bytes) }))
          })
      }
    }
    
    Net.pure(stream)
  }
}

-- Transform stream with chunked operations
let process_large_file: URI -> Net[Unit] = |uri| {
  do {
    let! stream = Http.get_stream(uri)
    
    -- All operations compose and fuse
    let processed = stream
      |> Stream.map(decompress_chunk)
      |> Stream.map(parse_lines)
      |> Stream.filter(is_valid_line)
      |> Stream.map(transform_data)
    
    -- Consume with backpressure
    let consumer = Consumer {
      init = FileHandle.open("output.txt"),
      consume = |handle, chunk| {
        IO.write(handle, chunk).map(|_| (handle, Continue))
      }
    }
    
    let! _ = run_stream(processed, consumer)
    Net.pure(Unit)
  }
}
```

The stream remains lazy until consumed. All transformations (`map`, `filter`) fuse into a single pass through the data. Chunks flow from producer to consumer only as fast as the consumer processes them, preventing memory overflow.

### Stream Combinators

Rich combinator library for stream transformation and composition:

```topos
module Stream {
  -- Basic combinators
  let filter: (A -> Bool) -> Stream[F, A] -> Stream[F, A] = ...
  let take: Int -> Stream[F, A] -> Stream[F, A] = ...
  let drop: Int -> Stream[F, A] -> Stream[F, A] = ...
  let concat: Stream[F, A] -> Stream[F, A] -> Stream[F, A] = ...
  
  -- Effectful operations
  let eval: F[A] -> Stream[F, A] = ...  -- Lift effect into stream
  let eval_map: (A -> F[B]) -> Stream[F, A] -> Stream[F, B] = ...
  
  -- Chunking operations
  let chunk_n: Int -> Stream[F, A] -> Stream[F, Chunk[A]] = ...
  let unchunk: Stream[F, Chunk[A]] -> Stream[F, A] = ...
  
  -- Stateful operations
  let scan: B -> (B -> A -> B) -> Stream[F, A] -> Stream[F, B] = ...
  let fold: B -> (B -> A -> B) -> Stream[F, A] -> F[B] = ...
  
  -- Parallel processing (leverages BEAM)
  let par_map: Int -> (A -> F[B]) -> Stream[F, A] -> Stream[F, B] = ...
  let par_eval_map: Int -> (A -> F[B]) -> Stream[F, A] -> Stream[F, B] = ...
  
  -- Resource safety
  let bracket: F[R] -> (R -> F[Unit]) -> (R -> Stream[F, A]) -> Stream[F, A] = ...
  
  -- Combining streams
  let merge: Stream[F, A] -> Stream[F, A] -> Stream[F, A] = ...  -- Interleave
  let zip: Stream[F, A] -> Stream[F, B] -> Stream[F, (A, B)] = ...
}
```

Parallel operations leverage BEAM's lightweight processes. `par_map(4, transform)` spawns 4 worker processes, distributes chunks to them, and merges results back into a single stream. This combines functional stream composition with BEAM's concurrency strength.

## Validation Framework Integration

### Applicative Functor for Request Validation

Applicative functors accumulate validation errors rather than short-circuiting on first failure:

```topos
-- Validated applicative accumulates errors
type Validated[E, A] = Valid(A) | Invalid(List[E])

impl Applicative[Validated[E, *]] where E: Semigroup {
  let pure: A -> Validated[E, A] = Valid
  
  let ap: Validated[E, A -> B] -> Validated[E, A] -> Validated[E, B] = 
    |vf, va| match (vf, va) {
      case (Valid(f), Valid(a)) -> Valid(f(a))
      case (Invalid(e1), Invalid(e2)) -> Invalid(e1 ++ e2)
      case (Invalid(e), _) -> Invalid(e)
      case (_, Invalid(e)) -> Invalid(e)
    }
}

-- Validation rules
type ValidationRule[A] = A -> Validated[ValidationError, A]

-- Example: validate HTTP request
module RequestValidation {
  let validate_method: ValidationRule[Request] = |req| {
    match req.method {
      case Get | Post | Put | Patch | Delete -> Valid(req)
      case Custom(m) if String.length(m) > 20 -> 
        Invalid([MethodTooLong(m)])
      case _ -> Valid(req)
    }
  }
  
  let validate_url: ValidationRule[Request] = |req| {
    if URI.is_valid(req.url) then
      Valid(req)
    else
      Invalid([InvalidURL(req.url)])
  }
  
  let validate_headers: ValidationRule[Request] = |req| {
    let invalid_headers = req.headers
      |> Map.filter(|name, _| not(Header.is_valid_name(name)))
      |> Map.keys()
    
    if List.is_empty(invalid_headers) then
      Valid(req)
    else
      Invalid(invalid_headers.map(InvalidHeader))
  }
  
  -- Combine validators using applicative
  let validate_request: Request -> Validated[ValidationError, Request] = |req| {
    -- All validators run and errors accumulate
    pure(identity)
      <*> validate_method(req)
      <*> validate_url(req)
      <*> validate_headers(req)
      |> Validated.map(|_| req)  -- Return original request if all pass
  }
}
```

The applicative instance ensures all validators run even if some fail, collecting all errors. This provides better user experience than monadic validation, which would stop at the first error.

### Response Validation

Similar pattern for validating responses:

```topos
-- Schema-based validation
type Schema[A] = {
  validate: JsonValue -> Validated[ValidationError, A]
}

-- Define schema for user response
let user_schema: Schema[User] = Schema {
  validate = |json| {
    -- Applicative validation of all fields
    pure(User.new)
      <*> field(json, "id", Int.schema)
      <*> field(json, "name", String.schema)
      <*> field(json, "email", Email.schema)
      <*> optional_field(json, "age", Int.schema)
  }
}

-- Integrate with HTTP client
let fetch_user: Int -> Net[Validated[ValidationError, User]] = |id| {
  do {
    let! response = Http.get("https://api.example.com/users/" ++ Int.to_string(id))
    let json = Json.parse(response.body)
    Net.pure(user_schema.validate(json))
  }
}
```

The validation framework integrates seamlessly with the HTTP client. Responses are parsed into validated domain types, with all validation errors collected and returned explicitly rather than thrown as exceptions.

## Error Handling Patterns

### Algebraic Error Types

Network errors are modeled as algebraic data types with full information:

```topos
type NetError =
  | TransportError(TransportReason)
  | TimeoutError(TimeoutType)
  | ValidationError(List[ValidationFailure])
  | HttpError(Int, Response)  -- Status code and full response
  | ProtocolError(ProtocolFailure)
  | PoolError(PoolFailure)

type TransportReason =
  | ConnectionRefused
  | ConnectionClosed
  | HostNotFound
  | SSLError(SSLFailure)

type TimeoutType =
  | ConnectTimeout
  | ReceiveTimeout
  | RequestTimeout(Duration)

-- Errors are explicit in types
let fetch: URI -> Net[Response]  -- Net[Response] = IO[Result[NetError, Response]]
```

Errors are values, not exceptions. Every function that can fail returns `Result[NetError, A]`, making failure modes explicit and forcing callers to handle them.

### Retry Logic with Exponential Backoff

Retry middleware uses the error step mechanism to recover from transient failures:

```topos
-- Retry configuration
type RetryPolicy = {
  max_attempts: Int,
  base_delay: Duration,
  max_delay: Duration,
  retry_on: NetError -> Bool
}

-- Default transient error detection
let is_transient: NetError -> Bool = {
  case TransportError(ConnectionRefused) -> true
  case TransportError(ConnectionClosed) -> true
  case TimeoutError(_) -> true
  case HttpError(status, _) if status >= 500 -> true
  case HttpError(429, _) -> true  -- Rate limit
  case _ -> false
}

-- Exponential backoff calculation
let backoff: Int -> Duration -> Duration -> Duration = |attempt, base, max| {
  let delay = base * (2 ** attempt)
  Duration.min(delay, max)
}

-- Retry step
let retry_step: RetryPolicy -> ErrorStep = |policy, (request, error)| {
  let attempt = Request.get_private(request, :retry_attempt)
    |> Option.default(0)
  
  if attempt >= policy.max_attempts then
    Net.pure((request, error))  -- Exhausted retries
  else if policy.retry_on(error) then {
    let delay = backoff(attempt, policy.base_delay, policy.max_delay)
    do {
      let! _ = IO.sleep(delay)
      let! _ = IO.println("Retrying after " ++ Duration.to_string(delay))
      
      -- Update request with incremented attempt
      let updated = Request.put_private(request, :retry_attempt, attempt + 1)
      
      -- Re-execute request (returns to request steps)
      let! response = Pipeline.execute(updated)
      Net.pure((updated, response))
    }
  } else
    Net.pure((request, error))  -- Non-transient error
}

-- Add to pipeline
let with_retry = Pipeline.append_error_step(pipeline, :retry, retry_step(default_policy))
```

The retry step inspects errors and decides whether to retry. On transient errors, it waits for the backoff duration, then returns a new request that will re-enter the pipeline. The attempt count is stored in the request's `private` metadata, enabling stateful retry logic without external state.

### Circuit Breaker Pattern

Prevent cascading failures by tracking error rates and opening the circuit when thresholds are exceeded:

```topos
-- Circuit breaker state
type CircuitState = 
  | Closed { failures: Int }
  | Open { opened_at: Instant }
  | HalfOpen { test_attempts: Int }

type CircuitBreakerConfig = {
  failure_threshold: Int,
  reset_timeout: Duration,
  half_open_max_attempts: Int
}

-- Circuit breaker step
let circuit_breaker: CircuitBreakerConfig -> RequestStep = |config, request| {
  do {
    let! state = CircuitBreaker.get_state(request.url.host)
    
    match state {
      case Open(opened_at) -> {
        let elapsed = Instant.now() - opened_at
        if elapsed >= config.reset_timeout then {
          let! _ = CircuitBreaker.transition(request.url.host, HalfOpen(0))
          Net.pure(request)  -- Allow request
        } else
          Net.error(CircuitOpenError(request.url.host))
      }
      
      case HalfOpen(attempts) ->
        if attempts < config.half_open_max_attempts then
          Net.pure(request)
        else
          Net.error(CircuitOpenError(request.url.host))
      
      case Closed(_) ->
        Net.pure(request)
    }
  }
}
```

The circuit breaker maintains state per host, transitioning between closed, open, and half-open states. It integrates as a request step, rejecting requests immediately when the circuit is open rather than attempting doomed network calls.

## BEAM Integration: Actors and Supervision

### Process-Based Connection Pool

Connection pools leverage BEAM processes for concurrency:

```topos
-- Pool supervisor manages connection workers
actor PoolSupervisor {
  state: {
    config: PoolConfig,
    workers: List[Process[ConnectionWorker]],
    available: Queue[Process[ConnectionWorker]],
    waiting: Queue[Process[Requester]]
  }
  
  -- Start pool with N workers
  init: PoolConfig -> Effect[PoolSupervisor] = |config| {
    let workers = List.replicate(config.size, |_| {
      Process.spawn_link(ConnectionWorker.new(config))
    })
    
    PoolSupervisor {
      config = config,
      workers = workers,
      available = Queue.from_list(workers),
      waiting = Queue.empty()
    }
  }
  
  -- Handle checkout request
  handle_message: Message -> Effect[Unit] = {
    case CheckoutRequest(requester) -> {
      match Queue.dequeue(state.available) {
        case Some((worker, rest)) -> {
          Process.send(requester, CheckoutSuccess(worker))
          state.available := rest
        }
        case None -> {
          state.waiting := Queue.enqueue(state.waiting, requester)
          -- Timeout handling
          Process.send_after(requester, CheckoutTimeout, config.pool_timeout)
        }
      }
    }
    
    case CheckinRequest(worker) -> {
      match Queue.dequeue(state.waiting) {
        case Some((requester, rest)) -> {
          Process.send(requester, CheckoutSuccess(worker))
          state.waiting := rest
        }
        case None -> {
          state.available := Queue.enqueue(state.available, worker)
        }
      }
    }
  }
}

-- Connection worker manages a single connection
actor ConnectionWorker {
  state: {
    config: PoolConfig,
    conn: Option[Connection],
    last_used: Instant
  }
  
  handle_message: Message -> Effect[Unit] = {
    case UseConnection(operation, reply_to) -> {
      do! {
        let! conn = get_or_connect()
        let! result = operation(conn)
        Process.send(reply_to, result)
        state.last_used := Instant.now()
      }
    }
    
    case CheckHealth -> {
      let idle_time = Instant.now() - state.last_used
      if idle_time > config.conn_max_idle_time then {
        do! {
          let! _ = close_connection()
          state.conn := None
        }
      }
    }
  }
}
```

Each pool is an actor that manages a collection of connection worker actors. Checkout/checkin operations are message-passing protocols. If no connection is available, the requester is queued and automatically notified when one becomes available.

### Supervision Trees for Reliability

Networking subsystems are organized in supervision trees:

```topos
-- Top-level HTTP client supervisor
actor HttpClient {
  children: [
    {
      id: :pool_manager,
      start: PoolManager.new(pool_configs),
      strategy: :one_for_one,
      restart: :permanent
    },
    {
      id: :metrics_collector,
      start: MetricsCollector.new(),
      strategy: :one_for_one,
      restart: :permanent
    }
  ]
}

-- Pool manager supervises individual pools
actor PoolManager {
  children: pool_configs.map(|host, config| {
    {
      id: host,
      start: PoolSupervisor.new(config),
      strategy: :one_for_one,
      restart: :transient  -- Restart if abnormal termination
    }
  })
}
```

When a connection worker crashes, the pool supervisor detects it and spawns a replacement. When a pool crashes, the pool manager restarts it. The entire system is fault-tolerant by construction, with explicit restart strategies for each component.

### Actor-Based Streaming

Streams can leverage BEAM processes for parallel processing:

```topos
-- Parallel stream processing with worker pool
let par_map: Int -> (A -> F[B]) -> Stream[F, A] -> Stream[F, B] = 
  |parallelism, f, stream| {
    Stream {
      step = |state| {
        do {
          -- Spawn worker pool
          let! workers = List.replicate(parallelism, |_| {
            Process.spawn(StreamWorker.new(f))
          })
          
          -- Distribute chunks to workers
          let! (next_state, maybe_chunk) = stream.step(state)
          match maybe_chunk {
            case None -> F.pure((next_state, None))
            case Some(chunk) -> {
              let! results = chunk.data
                |> distribute_to_workers(workers)
                |> collect_results()
              
              F.pure((next_state, Some(Chunk.from_list(results))))
            }
          }
        }
      }
    }
  }
```

Workers process elements in parallel, with results collected and merged back into a single stream. This parallelism is transparent to stream consumers—the type remains `Stream[F, B]` whether sequential or parallel.

## Complete Usage Example

Putting it all together, here's a comprehensive example demonstrating the full networking stack:

```topos
-- Define domain types
type User = { id: Int, name: String, email: String }
type Post = { id: Int, user_id: Int, title: String, body: String }

-- Configure HTTP client with plugins
let client = 
  Http.new(
    pools = Map.of([
      (Default, PoolConfig {
        strategy = Http1Pool { size = 50, count = 1 },
        conn_max_idle_time = Duration.minutes(5),
        conn_opts = ConnectionOptions.default()
      }),
      ((Https, "api.github.com", 443), PoolConfig {
        strategy = Http2Pool { count = 4 },
        conn_opts = ConnectionOptions {
          protocols = [:http2],
          ...ConnectionOptions.default()
        }
      })
    ])
  )
  |> attach_plugin(LoggingPlugin.new(:info))
  |> attach_plugin(RetryPlugin.new(max_attempts = 3))
  |> attach_plugin(RateLimitPlugin.new(limit = 100, window = Duration.seconds(60)))
  |> attach_plugin(MetricsPlugin.new())
  |> attach_plugin(CachingPlugin.new(ttl = Duration.minutes(5)))

-- Define schemas for validation
let user_schema = Schema {
  validate = |json| {
    pure(User.new)
      <*> field(json, "id", Int.schema)
      <*> field(json, "name", String.schema)
      <*> field(json, "email", Email.schema)
  }
}

let post_schema = Schema {
  validate = |json| {
    pure(Post.new)
      <*> field(json, "id", Int.schema)
      <*> field(json, "userId", Int.schema)
      <*> field(json, "title", String.schema)
      <*> field(json, "body", String.schema)
  }
}

-- Fetch user with validation
let fetch_user: Int -> Net[Validated[ValidationError, User]] = |id| {
  do {
    let! response = Http.get(
      client,
      url = "https://jsonplaceholder.typicode.com/users/" ++ Int.to_string(id)
    )
    
    let json = Json.parse(response.body)
    Net.pure(user_schema.validate(json))
  }
}

-- Fetch posts with streaming
let fetch_user_posts: Int -> Net[Stream[IO, Post]] = |user_id| {
  do {
    let! stream_response = Http.get_stream(
      client,
      url = "https://jsonplaceholder.typicode.com/posts?userId=" ++ Int.to_string(user_id)
    )
    
    -- Transform byte stream to post stream
    let post_stream = stream_response
      |> Stream.map(Bytes.to_string)
      |> Stream.map(Json.parse_array)
      |> Stream.map(|json_arr| json_arr.map(post_schema.validate))
      |> Stream.map(|validated_posts| {
          -- Filter to only valid posts
          validated_posts
          |> List.filter_map(Validated.to_option)
        })
      |> Stream.flat_map(Stream.from_list)
    
    Net.pure(post_stream)
  }
}

-- Parallel fetch with error handling
let fetch_user_with_posts: Int -> Net[(User, List[Post])] = |user_id| {
  do {
    -- Fetch user
    let! user_validated = fetch_user(user_id)
    let! user = match user_validated {
      case Valid(u) -> Net.pure(u)
      case Invalid(errors) -> Net.error(ValidationError(errors))
    }
    
    -- Stream posts and collect
    let! post_stream = fetch_user_posts(user_id)
    let! posts = Stream.fold([], List.cons, post_stream)
    
    Net.pure((user, posts))
  }
}

-- Upload with streaming request body
let upload_large_file: String -> Net[Response] = |filepath| {
  do {
    let! file_handle = IO.open_file(filepath, :read)
    
    -- Create stream from file
    let file_stream = Stream.bracket(
      acquire = IO.pure(file_handle),
      release = IO.close_file,
      use = |handle| {
        Stream.repeat_eval(IO.read_chunk(handle, chunk_size = 8192))
          |> Stream.take_while(|chunk| not(Bytes.is_empty(chunk)))
      }
    )
    
    -- Upload with streaming body
    let! response = Http.post(
      client,
      url = "https://api.example.com/upload",
      body = Body.Stream(file_stream),
      headers = Map.of([
        ("Content-Type", "application/octet-stream"),
        ("Transfer-Encoding", "chunked")
      ])
    )
    
    Net.pure(response)
  }
}

-- Custom middleware for circuit breaking
let with_circuit_breaker = 
  Pipeline.prepend_request_step(
    client.pipeline,
    :circuit_breaker,
    circuit_breaker(CircuitBreakerConfig {
      failure_threshold = 5,
      reset_timeout = Duration.seconds(30),
      half_open_max_attempts = 3
    })
  )

-- Main application
let main: IO[Unit] = {
  do {
    let! result = fetch_user_with_posts(1)
    
    match result {
      case Ok((user, posts)) -> {
        IO.println("User: " ++ user.name)
        IO.println("Posts: " ++ Int.to_string(List.length(posts)))
        posts
          |> List.take(3)
          |> List.iter(|post| IO.println("  - " ++ post.title))
      }
      case Err(error) -> {
        IO.println("Error: " ++ NetError.to_string(error))
      }
    }
  }
}
```

This example showcases the complete networking system: connection pooling with HTTP/1 and HTTP/2, composable plugins, validation with applicative functors, streaming with backpressure, error handling with retries and circuit breaking, and integration with BEAM processes.

## Performance Characteristics

**Connection pooling** reduces latency by reusing established connections. HTTP/1.1 pools maintain multiple connections per host, while HTTP/2 pools leverage multiplexing to handle concurrent requests on fewer connections. Pool configuration directly impacts throughput—tune `size` and `count` based on load patterns.

**Streaming** maintains constant memory usage regardless of payload size. Large files never fully load into memory; instead, they flow through in chunks. Backpressure prevents fast producers from overwhelming slow consumers, ensuring system stability under variable network conditions.

**Fusion** eliminates intermediate allocations in stream pipelines. Multiple `map` operations become a single transformation pass, multiple `filter` operations coalesce into one predicate check. The compiler optimizes functorial composition automatically, delivering hand-tuned performance from high-level code.

**Process-based parallelism** leverages BEAM's lightweight processes for concurrent operations. Parallel stream processing distributes work across multiple cores without explicit thread management. Actor-based pools handle thousands of concurrent connections efficiently.

## Conclusion

Topos networking demonstrates that category theory provides more than elegant abstractions—it delivers *practical, composable, type-safe networking* on BEAM. Functors transform data, monads sequence effects, arrows model protocols, and coalgebras capture stateful communication. These aren't academic curiosities but the foundation for robust, maintainable networked systems.

The design respects BEAM's strengths while adding functional rigor. Process supervision provides fault tolerance, message passing enables concurrency, and categorical types ensure correctness. When you compose middleware, the types guarantee it works. When you stream data, backpressure prevents overload. When operations fail, errors are values you handle explicitly.

This is networking done right: pure functions for transformations, controlled effects for I/O, algebraic types for errors, and BEAM processes for concurrency. The mathematics guarantees composability. The implementation delivers performance. The result is a networking layer that's simultaneously principled and pragmatic.
