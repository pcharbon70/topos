# Phase 7: Distribution Layer Integration

## Overview

This phase integrates Partisan's scalable distribution layer into Topos, demonstrating how category theory abstractions map to distributed systems programming. While Phases 1-6 focused on single-node BEAM execution, Phase 7 extends Topos to multi-node clusters with pluggable topologies, efficient message passing, and category-theoretic foundations. **Network topologies become categories**, **membership operations become functors**, and **messages become natural transformations**—making distributed programming type-safe and mathematically principled.

Partisan replaces Distributed Erlang's fixed full-mesh topology (O(n²) connections) with pluggable overlays: full-mesh for small clusters, HyParView for peer-to-peer at 2,000+ nodes, and client-server for IoT scenarios. Named channels eliminate head-of-line blocking, providing 12.5-38x throughput improvements. The integration demonstrates Topos's ability to handle complex real-world requirements while maintaining category theory foundations.

This phase runs for **4-6 weeks** as a post-PoC extension, proving that Topos's mathematical approach scales from single BEAM nodes to massive distributed clusters. By the end, Topos programs can leverage Partisan's battle-tested distribution (30M+ daily GPS transmissions across 300K vehicles) while maintaining compile-time guarantees through session types and effect tracking.

---

## 7.1 Partisan Integration
- [ ] **Section 7.1 Complete**

Integrate Partisan distribution layer as a dependency, establishing the foundation for distributed Topos programs. This includes FFI bindings to Partisan's Erlang API, initialization and configuration, and category-theoretic modeling of distributed concepts.

### 7.1.1 Partisan Dependencies and FFI
- [ ] **Task 7.1.1 Complete**

Add Partisan as a dependency and create FFI bindings for core Partisan operations. Topos programs will call Partisan through type-safe wrappers that preserve Topos's category theory abstractions.

- [ ] 7.1.1.1 Add Partisan v5.0.2 (or latest) as rebar3 dependency in project configuration
- [ ] 7.1.1.2 Create FFI module wrapping Partisan's `partisan` module functions (send, forward_message, nodes, spawn)
- [ ] 7.1.1.3 Create FFI module wrapping `partisan_peer_service` functions (join, leave, members, on_up, on_down)
- [ ] 7.1.1.4 Implement type-safe wrappers converting Topos types to Partisan's expected formats

### 7.1.2 Categorical Modeling
- [ ] **Task 7.1.2 Complete**

Define category theory abstractions for distributed concepts. Network topologies are categories (nodes as objects, channels as morphisms), membership operations are functors (transforming topology categories), and message passing is natural transformations (preserving categorical structure).

- [ ] 7.1.2.1 Define Topology trait as Category with nodes as objects and communication paths as morphisms
- [ ] 7.1.2.2 Model membership operations (join, leave) as functors between topology categories
- [ ] 7.1.2.3 Model message passing as natural transformations preserving compositional structure
- [ ] 7.1.2.4 Document categorical properties ensuring distributed operations respect category laws

### 7.1.3 Node Configuration and Initialization
- [ ] **Task 7.1.3 Complete**

Implement Topos-level API for configuring and starting Partisan nodes. This includes node specification, topology selection, and connection management integrated with Topos's effect system.

- [ ] 7.1.3.1 Define NodeSpec shape representing node name, listen addresses, channels, and metadata
- [ ] 7.1.3.2 Implement `partisan_init` effect operation for node initialization with topology selection
- [ ] 7.1.3.3 Implement `partisan_join` effect operation for joining clusters with remote node specs
- [ ] 7.1.3.4 Implement effect handlers executing actual Partisan initialization and configuration

### Unit Tests - Section 7.1
- [ ] **Unit Tests 7.1 Complete**
- [ ] Test FFI bindings correctly calling Partisan functions
- [ ] Test topology category definitions with composition and identity properties
- [ ] Test node configuration producing valid Partisan node specs
- [ ] Test node initialization and cluster joining with effect handlers

---

## 7.2 Named Channels
- [ ] **Section 7.2 Complete**

Implement named channels for logical grouping of TCP connections, eliminating head-of-line blocking and enabling traffic prioritization. Channels integrate with Topos's effect system, allowing developers to specify which channel to use for effectful operations.

### 7.2.1 Channel Configuration
- [ ] **Task 7.2.1 Complete**

Define channel configuration syntax and semantics. Channels have names, parallelism settings (connection count), and optional monotonic flag for automatic state deduplication.

- [ ] 7.2.1.1 Define Channel shape with name, parallelism, and monotonic flag fields
- [ ] 7.2.1.2 Implement channel configuration syntax in node specifications
- [ ] 7.2.1.3 Implement default channels (control, data, state_sync) with recommended settings
- [ ] 7.2.1.4 Validate channel configurations ensuring parallelism >= 1 and unique channel names

### 7.2.2 Channel-Aware Effects
- [ ] **Task 7.2.2 Complete**

Extend Process and custom effects with channel specifications. Effects can indicate which channel to use for operations, enabling fine-grained traffic management.

- [ ] 7.2.2.1 Add channel parameter to send/spawn effect operations: `send : Channel -> ProcessId -> Message -> Unit`
- [ ] 7.2.2.2 Implement channel resolution in effect handlers routing operations to specified channels
- [ ] 7.2.2.3 Implement default channel selection when no channel specified (use 'data' channel)
- [ ] 7.2.2.4 Implement monotonic channel optimization dropping intermediate messages for CRDT propagation

### 7.2.3 Channel Performance Monitoring
- [ ] **Task 7.2.3 Complete**

Provide introspection and monitoring for channel performance. Developers can query channel statistics and adjust configurations based on observed behavior.

- [ ] 7.2.3.1 Implement channel statistics effect operations returning throughput and latency metrics
- [ ] 7.2.3.2 Implement channel health checking detecting head-of-line blocking
- [ ] 7.2.3.3 Implement channel reconfiguration operations adjusting parallelism at runtime
- [ ] 7.2.3.4 Integrate channel metrics into standard logging and monitoring effects

### Unit Tests - Section 7.2
- [ ] **Unit Tests 7.2 Complete**
- [ ] Test channel configuration validation with valid and invalid settings
- [ ] Test channel-aware effects routing messages to correct channels
- [ ] Test monotonic channels dropping intermediate messages correctly
- [ ] Test channel statistics reporting accurate throughput and latency

---

## 7.3 Topology Selection
- [ ] **Section 7.3 Complete**

Implement pluggable topology selection enabling runtime choice of network overlay. Topos programs can choose full-mesh, HyParView peer-to-peer, client-server, or custom topologies based on deployment requirements.

### 7.3.1 Topology Abstractions
- [ ] **Task 7.3.1 Complete**

Define topology as abstract interface with implementations for each Partisan backend. Topologies implement Category trait, providing composition and identity operations.

- [ ] 7.3.1.1 Define Topology trait extending Category with membership and routing operations
- [ ] 7.3.1.2 Implement FullMesh topology for small clusters (60-200 nodes)
- [ ] 7.3.1.3 Implement HyParView topology for large peer-to-peer clusters (2,000+ nodes)
- [ ] 7.3.1.4 Implement ClientServer topology for IoT and mobile scenarios

### 7.3.2 Topology Configuration
- [ ] **Task 7.3.2 Complete**

Implement topology selection and configuration through effect system. Topology choice affects connection patterns, failure detection, and message routing.

- [ ] 7.3.2.1 Define TopologyConfig shape with topology type and parameters
- [ ] 7.3.2.2 Implement topology selection in node initialization effects
- [ ] 7.3.2.3 Implement topology-specific configuration (HyParView active/passive view sizes, etc.)
- [ ] 7.3.2.4 Validate topology configurations ensuring parameters within valid ranges

### 7.3.3 Dynamic Topology Switching
- [ ] **Task 7.3.3 Complete**

Enable runtime topology changes for adaptive systems. While not recommended for production, topology switching demonstrates Topos's flexibility and categorical foundations.

- [ ] 7.3.3.1 Implement topology switching effect operation migrating between topologies
- [ ] 7.3.3.2 Implement connection migration preserving active communications during switch
- [ ] 7.3.3.3 Implement topology switching validation ensuring safe transitions
- [ ] 7.3.3.4 Document topology switching trade-offs and recommended use cases

### Unit Tests - Section 7.3
- [ ] **Unit Tests 7.3 Complete**
- [ ] Test topology abstractions implementing Category laws (composition, identity)
- [ ] Test FullMesh topology with connection patterns for small clusters
- [ ] Test HyParView topology with active/passive views and membership shuffling
- [ ] Test ClientServer topology with hub-and-spoke patterns
- [ ] Test topology configuration validation with various parameter combinations

---

## 7.4 Distributed Actors
- [ ] **Section 7.4 Complete**

Extend actor model from Phase 5 to distributed setting. Actors can spawn on remote nodes, send messages across nodes, and maintain location transparency. Supervision trees span multiple nodes with distributed failure detection.

### 7.4.1 Remote Actor Spawning
- [ ] **Task 7.4.1 Complete**

Implement remote actor spawning allowing actors to start on specific nodes or let system choose placement. Remote spawning uses Partisan's spawn operation with location tracking.

- [ ] 7.4.1.1 Extend spawn effect operation with optional Node parameter for remote spawning
- [ ] 7.4.1.2 Implement remote spawn effect handler using Partisan's spawn function
- [ ] 7.4.1.3 Implement location tracking maintaining actor-to-node mappings
- [ ] 7.4.1.4 Implement location transparency ensuring RemoteRef handles encode node information

### 7.4.2 Cross-Node Message Passing
- [ ] **Task 7.4.2 Complete**

Implement message passing across nodes using Partisan's forward_message and channel routing. Messages use session types for protocol safety across distributed boundaries.

- [ ] 7.4.2.1 Extend send effect operation to handle remote ProcessIds using partisan:forward_message
- [ ] 7.4.2.2 Implement message serialization using Erlang's external term format
- [ ] 7.4.2.3 Implement channel specification for remote messages enabling traffic prioritization
- [ ] 7.4.2.4 Implement message delivery guarantees (at-most-once for unreliable, at-least-once for reliable)

### 7.4.3 Distributed Supervision
- [ ] **Task 7.4.3 Complete**

Extend supervision trees to span multiple nodes. Supervisors monitor remote actors using Partisan's failure detection and restart workers appropriately.

- [ ] 7.4.3.1 Extend supervisor spec to include remote node placement for workers
- [ ] 7.4.3.2 Implement remote monitoring using partisan:monitor for failure detection
- [ ] 7.4.3.3 Implement remote restart strategies handling node failures and restarts
- [ ] 7.4.3.4 Implement distributed supervision reporting showing cross-node actor hierarchies

### 7.4.4 Location Transparency and Discovery
- [ ] **Task 7.4.4 Complete**

Implement actor discovery and location transparency. Actors can find other actors by name or role without knowing physical location. Global registry maintains name-to-location mappings.

- [ ] 7.4.4.1 Implement global actor registry using Partisan's membership and metadata
- [ ] 7.4.4.2 Implement actor registration by name enabling location-independent addressing
- [ ] 7.4.4.3 Implement actor discovery by name or type searching across cluster
- [ ] 7.4.4.4 Implement automatic re-registration on actor migration or node changes

### Unit Tests - Section 7.4
- [ ] **Unit Tests 7.4 Complete**
- [ ] Test remote actor spawning on specific nodes
- [ ] Test cross-node message passing with serialization
- [ ] Test distributed supervision with remote monitoring and restarts
- [ ] Test location transparency with actor discovery and migration

---

## 7.5 Session Types for Distributed Protocols
- [ ] **Section 7.5 Complete**

Extend session types from Phase 8 (Advanced Concurrency) to distributed setting. Session types ensure protocol safety across node boundaries with compile-time guarantees about message ordering and choice coherence.

### 7.5.1 Distributed Session Establishment
- [ ] **Task 7.5.1 Complete**

Implement session establishment across nodes using Partisan channels. Sessions encode protocol in types and verify conformance at compile time.

- [ ] 7.5.1.1 Extend session type syntax with location annotations for remote endpoints
- [ ] 7.5.1.2 Implement session establishment effect creating bidirectional channels between nodes
- [ ] 7.5.1.3 Implement session negotiation ensuring both endpoints agree on protocol
- [ ] 7.5.1.4 Implement session serialization for cross-node communication

### 7.5.2 Protocol Verification
- [ ] **Task 7.5.2 Complete**

Verify distributed protocols satisfy session type contracts. Compiler checks that all branches send/receive expected messages in correct order across node boundaries.

- [ ] 7.5.2.1 Implement distributed session type checking verifying protocol conformance
- [ ] 7.5.2.2 Implement choice coherence checking ensuring matching select/branch across nodes
- [ ] 7.5.2.3 Implement ordering verification preventing message reordering violations
- [ ] 7.5.2.4 Implement session completion checking ensuring sessions properly close or recurse

### Unit Tests - Section 7.5
- [ ] **Unit Tests 7.5 Complete**
- [ ] Test distributed session establishment between nodes
- [ ] Test session type checking catching protocol violations
- [ ] Test choice coherence across node boundaries
- [ ] Test session completion with proper resource cleanup

---

## 7.6 Integration Tests
- [ ] **Section 7.6 Complete**

Integration tests validate the complete distribution layer with realistic multi-node programs. Tests use multiple BEAM nodes (via distributed Erlang or test framework) demonstrating Partisan's capabilities through Topos's API.

### 7.6.1 Multi-Node Cluster Setup
- [ ] **Task 7.6.1 Complete**

Test cluster formation and topology configuration with multiple nodes. Verify nodes join correctly, topologies establish expected connection patterns, and membership updates propagate.

- [ ] 7.6.1.1 Test FullMesh cluster with 10 nodes verifying full connectivity
- [ ] 7.6.1.2 Test HyParView cluster with 50 nodes verifying partial mesh with active/passive views
- [ ] 7.6.1.3 Test ClientServer cluster with 5 servers and 20 clients verifying hub-and-spoke
- [ ] 7.6.1.4 Test membership changes (join/leave) propagating correctly across topologies

### 7.6.2 Distributed Actor Applications
- [ ] **Task 7.6.2 Complete**

Build complete distributed applications demonstrating actors, supervision, and message passing across nodes. Examples: distributed key-value store, chat server, distributed game server.

- [ ] 7.6.2.1 Implement distributed key-value store with partitioned data across nodes
- [ ] 7.6.2.2 Implement chat server with rooms spanning multiple nodes
- [ ] 7.6.2.3 Implement distributed game server with player actors and game state actors
- [ ] 7.6.2.4 Test applications with node failures and recovery

### 7.6.3 Performance Benchmarks
- [ ] **Task 7.6.3 Complete**

Benchmark Partisan performance through Topos demonstrating throughput improvements from named channels and topology optimizations.

- [ ] 7.6.3.1 Benchmark single-channel vs multi-channel throughput
- [ ] 7.6.3.2 Benchmark FullMesh vs HyParView latency and throughput at various cluster sizes
- [ ] 7.6.3.3 Benchmark monotonic channel state propagation with CRDT workloads
- [ ] 7.6.3.4 Compare Topos/Partisan to native Distributed Erlang performance

### 7.6.4 Category Theory Validation
- [ ] **Task 7.6.4 Complete**

Validate that distributed operations preserve category theory properties. Verify composition laws hold across nodes, functors preserve structure, and natural transformations commute.

- [ ] 7.6.4.1 Test topology categories satisfy composition and identity laws
- [ ] 7.6.4.2 Test membership functors preserve categorical structure
- [ ] 7.6.4.3 Test message passing natural transformations commute correctly
- [ ] 7.6.4.4 Test session types preserve protocol structure across distribution

---

## Success Criteria

1. **Partisan Integration**: Successfully integrate Partisan v5.0.2+ with type-safe FFI bindings
2. **Topology Support**: Implement FullMesh, HyParView, and ClientServer topologies with correct connection patterns
3. **Named Channels**: Support multi-channel communication with measurable performance improvements
4. **Distributed Actors**: Remote spawning, cross-node messaging, and distributed supervision working correctly
5. **Session Types**: Distributed session types providing compile-time protocol verification
6. **Performance**: Achieve >10x throughput improvement with named channels compared to single-channel
7. **Category Theory**: All distributed operations preserve category laws (composition, identity, etc.)
8. **Integration**: Complete distributed applications (KV store, chat, game) running successfully

## Provides Foundation For

- Geographic distribution with multi-datacenter deployments
- Edge computing with client-server topologies
- Large-scale IoT platforms with HyParView
- Distributed databases and storage systems
- Real-time distributed applications with session types

## Key Outputs

- Partisan FFI bindings for Topos
- Named channel API with effect integration
- Topology selection and configuration system
- Distributed actor runtime extending Phase 5
- Session types for distributed protocols
- Performance benchmarks demonstrating scalability
- Example distributed applications
- Documentation on category-theoretic foundations for distribution

---

## Timeline Note

This phase is estimated at **4-6 weeks** and represents a significant post-PoC extension. The variability comes from:

- 4 weeks: Minimal implementation with FullMesh topology only, basic channels, simple distributed actors
- 6 weeks: Complete implementation with all topologies, full channel features, distributed supervision, session types

Teams may choose to implement Phase 7 incrementally, starting with FullMesh and basic distribution before adding advanced topologies and session types.
