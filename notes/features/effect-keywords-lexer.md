# Effect Keywords Integration - Feature Planning Document

## Metadata

**Feature ID**: Effect-Keywords-Lexer
**Related Task**: 1.1.5 - Effect Syntax Support
**Phase**: Phase 1 - Core Language Infrastructure
**Section**: 1.1 - Lexer and Parser
**Estimated Duration**: 1-2 days
**Dependencies**:
- Task 1.1.1 (Token Recognition) - COMPLETED
- Phase 01 updates for algebraic effect system
**Created**: 2025-11-09

---

## Problem Statement

Phase 01 has been updated to include a **minimal viable algebraic effect system** as a core feature of the Topos language. The effect system enables category-theoretic purity while meeting BEAM pragmatism through algebraic effects. However, the current lexer implementation (Task 1.1.1) does not recognize effect-related keywords needed for this system.

### Missing Functionality

The lexer currently lacks support for five critical effect-related keywords:

1. **`effect`** - Declares an algebraic effect with named operations
   ```topos
   effect FileIO {
     operation read(path: String): String
     operation write(path: String, content: String): Unit
   }
   ```

2. **`operation`** - Declares operations within an effect declaration
   ```topos
   operation read(path: String): String
   ```

3. **`perform`** - Invokes an effectful operation
   ```topos
   perform FileIO.read("/etc/config")
   ```

4. **`try`** - Begins an effect handler block (with `with` clause)
   ```topos
   try {
     perform FileIO.read(path)
   } with FileIO {
     read(p) -> "mock content"
   }
   ```

5. **`with`** - Introduces handler cases in a try/with block
   ```topos
   try { ... } with EffectName { ... }
   ```

### Impact on Compilation Pipeline

Without these keywords, the lexer cannot tokenize effect-related syntax, which blocks:
- **Task 1.1.5** (Effect Syntax Support) - Parser cannot be implemented
- **Section 1.2** (Core Type System) - Effect tracking requires effect AST nodes
- **Section 1.3** (Code Generation) - Effect runtime requires compiled effect handlers
- **Section 1.4** (Integration Tests) - Cannot test effectful programs end-to-end

### Current State

The lexer implementation (`topos_lexer.xrl`) successfully handles:
- 17 existing keywords (shape, flow, match, where, let, in, do, end, if, then, else, case, of, when, module, import, export, exports, as, qualified, private, trait, instance, forall, actor, supervisor)
- 14 two-character operators
- 6 single-character operators
- Delimiters, numeric literals, string literals, identifiers, comments

**What's Missing**: Effect-related keywords are not defined in the Rules section.

**Target State**: The lexer recognizes all five effect keywords and produces appropriate tokens for parser consumption, enabling Task 1.1.5 to proceed with effect syntax parsing.

---

## Solution Overview

We will extend the existing `topos_lexer.xrl` leex definition file by adding five new keyword rules in the Rules section. This is a minimal, non-invasive change that follows the established pattern for keyword recognition.

### High-Level Approach

1. **Add keyword rules** to `topos_lexer.xrl` in the Rules section after existing keywords
2. **Follow existing patterns** - use identical token generation structure as other keywords
3. **Maintain alphabetical ordering** - place keywords in appropriate position for readability
4. **Test thoroughly** - ensure no conflicts with existing tokens or identifiers

### Why This Approach

- **Consistency**: Uses the same leex pattern as all existing keywords
- **Minimal Risk**: Only adds new rules, doesn't modify existing ones
- **No Breaking Changes**: Existing tokenization behavior is unchanged
- **Future-Proof**: Establishes foundation for parser implementation (Task 1.1.5)

### Token Format

Each keyword will generate a token tuple following the standard format:

```erlang
{KeywordAtom, LineNumber}
```

Examples:
- `effect` → `{effect, TokenLine}`
- `operation` → {operation, TokenLine}`
- `perform` → `{perform, TokenLine}`
- `try` → `{'try', TokenLine}`
- `with` → `{with, TokenLine}`

Note: `try` is an Erlang reserved word, so it must be quoted as `'try'` in the token tuple.

---

## Technical Details

### File Locations

Only one file needs modification:

```
topos/
├── src/
│   ├── compiler/
│   │   ├── lexer/
│   │   │   ├── topos_lexer.xrl       # MODIFY: Add 5 keyword rules
│   │   │   └── topos_lexer_gen.erl   # AUTO-GENERATED: Will be regenerated
└── test/
    └── topos_lexer_tests.erl         # MODIFY: Add test cases for new keywords
```

Note: `topos_lexer_gen.erl` is auto-generated by leex from `topos_lexer.xrl` and should not be edited manually.

### Specific Code Changes

#### Change 1: Add Effect Keywords to Rules Section

**File**: `/home/ducky/code/topos/src/compiler/lexer/topos_lexer.xrl`

**Location**: Lines 51-77 (Keywords section of Rules)

**Current Code** (excerpt):
```erlang
%% Keywords
shape : {token, {shape, TokenLine}}.
flow : {token, {flow, TokenLine}}.
match : {token, {match, TokenLine}}.
where : {token, {where, TokenLine}}.
let : {token, {'let', TokenLine}}.
in : {token, {'in', TokenLine}}.
do : {token, {'do', TokenLine}}.
end : {token, {'end', TokenLine}}.
if : {token, {'if', TokenLine}}.
then : {token, {'then', TokenLine}}.
else : {token, {'else', TokenLine}}.
case : {token, {'case', TokenLine}}.
of : {token, {'of', TokenLine}}.
when : {token, {'when', TokenLine}}.
module : {token, {'module', TokenLine}}.
import : {token, {'import', TokenLine}}.
export : {token, {'export', TokenLine}}.
exports : {token, {exports, TokenLine}}.
as : {token, {as, TokenLine}}.
qualified : {token, {qualified, TokenLine}}.
private : {token, {private, TokenLine}}.
trait : {token, {trait, TokenLine}}.
instance : {token, {instance, TokenLine}}.
forall : {token, {forall, TokenLine}}.
actor : {token, {actor, TokenLine}}.
supervisor : {token, {supervisor, TokenLine}}.
```

**Modified Code**:
```erlang
%% Keywords
shape : {token, {shape, TokenLine}}.
flow : {token, {flow, TokenLine}}.
match : {token, {match, TokenLine}}.
where : {token, {where, TokenLine}}.
let : {token, {'let', TokenLine}}.
in : {token, {'in', TokenLine}}.
do : {token, {'do', TokenLine}}.
end : {token, {'end', TokenLine}}.
if : {token, {'if', TokenLine}}.
then : {token, {'then', TokenLine}}.
else : {token, {'else', TokenLine}}.
case : {token, {'case', TokenLine}}.
of : {token, {'of', TokenLine}}.
when : {token, {'when', TokenLine}}.
module : {token, {'module', TokenLine}}.
import : {token, {'import', TokenLine}}.
export : {token, {'export', TokenLine}}.
exports : {token, {exports, TokenLine}}.
as : {token, {as, TokenLine}}.
qualified : {token, {qualified, TokenLine}}.
private : {token, {private, TokenLine}}.
trait : {token, {trait, TokenLine}}.
instance : {token, {instance, TokenLine}}.
forall : {token, {forall, TokenLine}}.
actor : {token, {actor, TokenLine}}.
supervisor : {token, {supervisor, TokenLine}}.
effect : {token, {effect, TokenLine}}.
operation : {token, {operation, TokenLine}}.
perform : {token, {perform, TokenLine}}.
try : {token, {'try', TokenLine}}.
with : {token, {with, TokenLine}}.
```

**Rationale**:
- Adds five new lines at the end of the keywords section
- Follows identical pattern to existing keywords
- Maintains consistency in token structure
- `try` uses quoted atom `'try'` because it's an Erlang reserved word
- Other keywords use unquoted atoms as they don't conflict with Erlang

#### Change 2: Regenerate Lexer Module

**Command**:
```bash
cd /home/ducky/code/topos
./rebar3 compile
```

This will automatically invoke leex to regenerate `topos_lexer_gen.erl` from the modified `topos_lexer.xrl`.

**Expected Output**:
- Successful compilation with no warnings
- Updated `src/compiler/lexer/topos_lexer_gen.erl` (auto-generated)
- Updated `ebin/topos_lexer_gen.beam` (compiled bytecode)

### Testing Strategy

#### Unit Tests

**File**: `/home/ducky/code/topos/test/topos_lexer_tests.erl`

Add test cases to verify each new keyword is tokenized correctly:

```erlang
%% Test effect keyword recognition
effect_keyword_test() ->
    Input = "effect FileIO",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([{effect, 1}, {upper_ident, 1, "FileIO"}], Tokens).

%% Test operation keyword recognition
operation_keyword_test() ->
    Input = "operation read",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([{operation, 1}, {lower_ident, 1, "read"}], Tokens).

%% Test perform keyword recognition
perform_keyword_test() ->
    Input = "perform FileIO.read",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([
        {perform, 1},
        {upper_ident, 1, "FileIO"},
        {dot, 1},
        {lower_ident, 1, "read"}
    ], Tokens).

%% Test try keyword recognition
try_keyword_test() ->
    Input = "try { x }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([
        {'try', 1},
        {lbrace, 1},
        {lower_ident, 1, "x"},
        {rbrace, 1}
    ], Tokens).

%% Test with keyword recognition
with_keyword_test() ->
    Input = "with FileIO { }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([
        {with, 1},
        {upper_ident, 1, "FileIO"},
        {lbrace, 1},
        {rbrace, 1}
    ], Tokens).

%% Test complete effect syntax tokenization
complete_effect_syntax_test() ->
    Input = "effect FileIO { operation read }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([
        {effect, 1},
        {upper_ident, 1, "FileIO"},
        {lbrace, 1},
        {operation, 1},
        {lower_ident, 1, "read"},
        {rbrace, 1}
    ], Tokens).

%% Test complete handler syntax tokenization
complete_handler_syntax_test() ->
    Input = "try { perform E.op } with E { }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    Expected = [
        {'try', 1},
        {lbrace, 1},
        {perform, 1},
        {upper_ident, 1, "E"},
        {dot, 1},
        {lower_ident, 1, "op"},
        {rbrace, 1},
        {with, 1},
        {upper_ident, 1, "E"},
        {lbrace, 1},
        {rbrace, 1}
    ],
    ?assertEqual(Expected, Tokens).

%% Test that keywords are not recognized as identifiers
effect_not_identifier_test() ->
    Input = "effect",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([{effect, 1}], Tokens),
    ?assertNotEqual([{lower_ident, 1, "effect"}], Tokens).

%% Test multi-line effect declaration
multiline_effect_test() ->
    Input = "effect FileIO\n  operation read",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertEqual([
        {effect, 1},
        {upper_ident, 1, "FileIO"},
        {operation, 2},
        {lower_ident, 2, "read"}
    ], Tokens).

%% Test that partial keyword matches don't tokenize as keywords
partial_keyword_test() ->
    %% "effective" should be identifier, not keyword
    {ok, Tokens1} = topos_lexer:tokenize("effective"),
    ?assertEqual([{lower_ident, 1, "effective"}], Tokens1),

    %% "performance" should be identifier, not keyword
    {ok, Tokens2} = topos_lexer:tokenize("performance"),
    ?assertEqual([{lower_ident, 1, "performance"}], Tokens2),

    %% "trial" should be identifier, not keyword
    {ok, Tokens3} = topos_lexer:tokenize("trial"),
    ?assertEqual([{lower_ident, 1, "trial"}], Tokens3).
```

#### Integration Tests

Test realistic effect syntax examples that will be used in Phase 1:

```erlang
%% Test complete effect declaration from Phase 1 documentation
phase1_effect_declaration_test() ->
    Input = "effect FileIO { operation read(path: String): String }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    %% Verify all tokens are present (detailed assertion)
    ?assertMatch([
        {effect, _},
        {upper_ident, _, "FileIO"},
        {lbrace, _},
        {operation, _},
        {lower_ident, _, "read"},
        {lparen, _},
        {lower_ident, _, "path"},
        {colon, _},
        {upper_ident, _, "String"},
        {rparen, _},
        {colon, _},
        {upper_ident, _, "String"},
        {rbrace, _}
    ], Tokens).

%% Test perform expression from Phase 1 documentation
phase1_perform_test() ->
    Input = "perform FileIO.read(path)",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    ?assertMatch([
        {perform, _},
        {upper_ident, _, "FileIO"},
        {dot, _},
        {lower_ident, _, "read"},
        {lparen, _},
        {lower_ident, _, "path"},
        {rparen, _}
    ], Tokens).

%% Test try/with handler from Phase 1 documentation
phase1_handler_test() ->
    Input = "try { perform FileIO.read(p) } with FileIO { read(path) -> content }",
    {ok, Tokens} = topos_lexer:tokenize(Input),
    %% Verify structure without exhaustive token-by-token check
    ?assert(lists:member({'try', 1}, Tokens)),
    ?assert(lists:member({with, 1}, Tokens)),
    ?assert(lists:member({perform, 1}, Tokens)),
    ?assert(lists:member({arrow, 1}, Tokens)).
```

### Edge Cases and Validation

#### Edge Case 1: Keywords vs Identifiers

**Problem**: Ensure keywords are recognized as keywords, not identifiers.

**Solution**: Leex processes rules in order. Since keyword rules appear before the general `{LOWER_IDENT}` and `{UPPER_IDENT}` rules in the file, they take precedence. This is already the established pattern.

**Test**: `partial_keyword_test()` verifies that "effective" tokenizes as identifier, while "effect" tokenizes as keyword.

#### Edge Case 2: Reserved Word Conflict (`try`)

**Problem**: `try` is an Erlang reserved word and cannot be used as an unquoted atom.

**Solution**: Use quoted atom syntax `'try'` in the token tuple. This is already done for other Erlang reserved words like `'let'`, `'in'`, `'do'`, `'end'`, `'if'`, `'then'`, `'else'`, `'case'`, `'of'`, `'when'`, `'module'`, `'import'`, `'export'`, `'and'`, `'or'`.

**Test**: `try_keyword_test()` verifies token is `{'try', 1}`.

#### Edge Case 3: Multi-line Effect Syntax

**Problem**: Ensure line tracking works correctly across multi-line effect declarations.

**Solution**: Leex's `TokenLine` variable automatically tracks line numbers based on newline characters in input. No additional implementation needed.

**Test**: `multiline_effect_test()` verifies line numbers increment correctly.

#### Edge Case 4: Keyword Substring Matching

**Problem**: Ensure "performance" doesn't match "perform", "trial" doesn't match "try", etc.

**Solution**: Leex uses longest match rule. The identifier pattern `{LOWER_IDENT} = {LOWER}{IDENT_CHAR}*` will match the full word. Since "performance" is not exactly "perform", it will be tokenized as identifier.

**Test**: `partial_keyword_test()` verifies proper tokenization of similar words.

---

## Implementation Plan

This is a straightforward implementation with clear steps and minimal risk.

### Step 1: Update Lexer Definition File

**Duration**: 15 minutes

**Actions**:
1. Open `/home/ducky/code/topos/src/compiler/lexer/topos_lexer.xrl`
2. Navigate to the Keywords section (lines 51-77)
3. Add five new keyword rules at the end of the section:
   ```erlang
   effect : {token, {effect, TokenLine}}.
   operation : {token, {operation, TokenLine}}.
   perform : {token, {perform, TokenLine}}.
   try : {token, {'try', TokenLine}}.
   with : {token, {with, TokenLine}}.
   ```
4. Save the file

**Success Criteria**:
- File saved without syntax errors
- New rules follow identical pattern to existing keywords
- `try` uses quoted atom `'try'`

### Step 2: Regenerate Lexer Module

**Duration**: 5 minutes

**Actions**:
1. Run `./rebar3 compile` from repository root
2. Verify successful compilation with no warnings
3. Check that `src/compiler/lexer/topos_lexer_gen.erl` was regenerated (check timestamp)

**Success Criteria**:
- Compilation succeeds
- No leex warnings or errors
- Generated file timestamp is updated

### Step 3: Add Unit Tests

**Duration**: 45 minutes

**Actions**:
1. Open `/home/ducky/code/topos/test/topos_lexer_tests.erl`
2. Add all unit test cases from Testing Strategy section above
3. Run tests: `./rebar3 eunit --module=topos_lexer_tests`

**Success Criteria**:
- All 10 new test cases pass
- All existing test cases still pass (regression check)
- No test failures or errors

### Step 4: Add Integration Tests

**Duration**: 30 minutes

**Actions**:
1. Add integration test cases from Testing Strategy section
2. Test realistic effect syntax from Phase 1 documentation
3. Run full test suite: `./rebar3 eunit`

**Success Criteria**:
- All 3 integration tests pass
- Tests verify complete effect declaration, perform expression, and try/with handler
- Full test suite passes

### Step 5: Manual Verification

**Duration**: 15 minutes

**Actions**:
1. Start Erlang shell: `./rebar3 shell`
2. Manually test each keyword:
   ```erlang
   {ok, T1} = topos_lexer:tokenize("effect").
   {ok, T2} = topos_lexer:tokenize("operation").
   {ok, T3} = topos_lexer:tokenize("perform").
   {ok, T4} = topos_lexer:tokenize("try").
   {ok, T5} = topos_lexer:tokenize("with").
   ```
3. Test complete effect syntax:
   ```erlang
   {ok, Tokens} = topos_lexer:tokenize("effect FileIO { operation read }").
   ```
4. Verify output matches expectations

**Success Criteria**:
- All keywords tokenize correctly in REPL
- Complex effect syntax produces expected token stream
- No errors or unexpected tokens

### Step 6: Documentation Update

**Duration**: 15 minutes

**Actions**:
1. Update any relevant documentation that lists Topos keywords
2. Ensure effect keywords are mentioned in appropriate contexts
3. Note that Task 1.1.5 can now proceed with parser implementation

**Success Criteria**:
- Documentation accurately reflects new keywords
- Clear path forward for Task 1.1.5

---

## Success Criteria

The implementation is complete when all of the following are true:

### Functional Criteria

1. **Keyword Recognition**: All five effect keywords tokenize correctly
   - `effect` → `{effect, LineNumber}`
   - `operation` → `{operation, LineNumber}`
   - `perform` → `{perform, LineNumber}`
   - `try` → `{'try', LineNumber}`
   - `with` → `{with, LineNumber}`

2. **Identifier Distinction**: Keywords are not recognized as identifiers
   - "effect" is keyword, "effective" is identifier
   - "perform" is keyword, "performance" is identifier
   - "try" is keyword, "trial" is identifier

3. **Complete Syntax Support**: Realistic effect syntax tokenizes correctly
   - Effect declarations: `effect FileIO { operation read }`
   - Perform expressions: `perform FileIO.read(path)`
   - Try/with handlers: `try { ... } with E { ... }`

4. **Line Tracking**: Multi-line effect syntax tracks line numbers correctly

### Quality Criteria

5. **Test Coverage**: All new functionality has comprehensive tests
   - 10+ unit tests covering each keyword individually
   - 3+ integration tests covering realistic syntax
   - All tests pass without failures

6. **Regression Safety**: Existing functionality is unchanged
   - All existing lexer tests pass
   - No changes to existing token types
   - No performance degradation

7. **Code Quality**: Implementation follows established patterns
   - Consistent with existing keyword rules
   - No code duplication
   - Clear and maintainable

### Readiness Criteria

8. **Task 1.1.5 Readiness**: Parser implementation can proceed
   - All effect-related tokens are available
   - Token format is parser-friendly
   - Documentation is clear for parser developers

9. **Phase 1 Alignment**: Implementation matches Phase 1 requirements
   - Supports effect declarations (Task 1.1.5.1)
   - Supports perform expressions (Task 1.1.5.2)
   - Supports try/with handlers (Task 1.1.5.3)
   - Foundation for effect annotations (Task 1.1.5.4)

---

## Dependencies and Blockers

### Dependencies

1. **Task 1.1.1 (Token Recognition)** - COMPLETED
   - Provides working lexer infrastructure
   - Establishes keyword tokenization patterns
   - No changes needed, just extension

2. **Phase 01 Documentation** - COMPLETED
   - Defines effect system requirements
   - Specifies effect syntax
   - Provides examples for testing

### No Blockers

This implementation has no blockers:
- All prerequisite tasks are complete
- No external dependencies required
- No architectural decisions pending
- No resource constraints

### Enables Future Work

This implementation enables:

1. **Task 1.1.5 (Effect Syntax Support)** - Parser implementation for effects
2. **Section 1.2 (Type System)** - Effect tracking and type-and-effect inference
3. **Section 1.3 (Code Generation)** - Effect runtime compilation
4. **Section 1.4 (Integration Tests)** - End-to-end effectful program testing

---

## Risk Assessment

### Risk Level: LOW

This is a low-risk implementation:

### Low Risk Factors

1. **Minimal Code Changes**: Only adds 5 lines to existing file
2. **Established Patterns**: Follows existing keyword implementation exactly
3. **No Breaking Changes**: Zero impact on existing functionality
4. **Comprehensive Testing**: 13+ test cases validate behavior
5. **Reversibility**: Changes can be easily reverted if needed
6. **Clear Scope**: Well-defined boundaries, no scope creep risk

### Potential Issues and Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Reserved word conflict with `try` | Already handled | None | Using `'try'` quoted atom (existing pattern) |
| Keyword collision with identifiers | Very Low | Low | Leex longest match rule + test coverage |
| Compilation errors from leex | Very Low | Low | Syntax is identical to working keywords |
| Test failures | Low | Low | Comprehensive test suite + manual verification |
| Breaking existing tests | Very Low | Medium | Run full regression suite |

---

## Alternative Approaches Considered

### Alternative 1: Implement as Contextual Keywords

**Description**: Make effect keywords contextual - they would only be recognized as keywords in specific contexts (e.g., "effect" only at module level).

**Pros**:
- Allows using these words as identifiers elsewhere
- More flexible language design

**Cons**:
- Significantly more complex lexer logic
- Requires parser cooperation (lookahead/context tracking)
- Not consistent with existing Topos keyword approach
- Harder to maintain and debug

**Decision**: Rejected - Not worth the complexity for minimal benefit. All other Topos keywords are unconditional.

### Alternative 2: Use Different Keywords

**Description**: Choose different keywords that don't conflict with common words (e.g., `eff`, `op`, `do_effect`, `handle`).

**Pros**:
- Potentially less collision with identifier naming
- Could be shorter/more concise

**Cons**:
- Phase 1 documentation already uses current keywords
- Industry standard (Koka, Eff, Frank) uses similar keywords
- "effect" and "perform" are self-documenting and clear
- Would require updating all Phase 1 documentation

**Decision**: Rejected - Current keywords are well-chosen, align with research, and are documented in Phase 1.

### Alternative 3: Reuse Existing Keywords

**Description**: Reuse existing keywords in effect contexts (e.g., use `shape` for effects, `do` for perform).

**Pros**:
- Fewer total keywords in language
- Smaller lexer footprint

**Cons**:
- Confusing and ambiguous
- Poor developer experience
- Makes parser more complex
- Violates principle of least surprise

**Decision**: Rejected - Effect system is first-class and deserves dedicated keywords.

### Chosen Approach: Direct Keyword Addition

**Why**:
- Simplest implementation
- Consistent with existing patterns
- Aligns with Phase 1 documentation
- Clear and unambiguous
- Enables future work without friction

---

## Future Enhancements

This implementation provides the foundation for future effect system work. Potential future enhancements include:

### Short-Term (Phase 1)

1. **Effect Annotation Syntax** (Task 1.1.5.4)
   - Add `/` operator for effect sets in type signatures
   - Example: `String / {FileIO, Process}`
   - Already supported by existing `/` (slash) operator

2. **Handler Case Syntax**
   - Parser will use existing `->` and `|` operators
   - No additional lexer changes needed

### Long-Term (Phase 6+)

3. **Effect Polymorphism Keywords**
   - `ρ` (rho) for effect variables (if using Greek letters)
   - `effect_row` for effect row polymorphism
   - Deferred until advanced effect features

4. **Effect Pragmas**
   - `@pure` annotation for pure functions
   - `@no_inline` for effect handler optimization
   - Deferred until optimization phase

5. **Standard Effect Keywords**
   - `IO`, `State`, `Exception` as builtin effects
   - Likely implemented as library, not keywords
   - Deferred to standard library development

---

## Conclusion

This feature planning document provides a comprehensive blueprint for integrating algebraic effect system keywords into the Topos lexer. The implementation is:

- **Low Risk**: Minimal changes, established patterns
- **High Value**: Unblocks critical effect system work
- **Well-Scoped**: Clear boundaries and success criteria
- **Thoroughly Planned**: Detailed implementation steps and testing

**Estimated Total Time**: 2 hours and 5 minutes

**Recommended Timeline**:
- Day 1 (Morning): Steps 1-3 (Implementation and unit tests)
- Day 1 (Afternoon): Steps 4-6 (Integration tests, verification, documentation)

Upon completion, Task 1.1.5 (Effect Syntax Support) can proceed with parser implementation, advancing the Phase 1 effect system integration.

---

## References

### Phase 1 Documentation

- **Phase 01 Plan**: `/home/ducky/code/topos/notes/planning/proof-of-concept/phase-01.md`
  - Section 1.1.5: Effect Syntax Support
  - Section 1.2: Core Type System (Effect Tracking)
  - Section 1.3.5: Effect Runtime System

### Implementation Files

- **Lexer Definition**: `/home/ducky/code/topos/src/compiler/lexer/topos_lexer.xrl`
- **Lexer Module**: `/home/ducky/code/topos/src/compiler/lexer/topos_lexer.erl`
- **Generated Lexer**: `/home/ducky/code/topos/src/compiler/lexer/topos_lexer_gen.erl`
- **Test Suite**: `/home/ducky/code/topos/test/topos_lexer_tests.erl`

### Related Tasks

- **Task 1.1.1**: Token Recognition (COMPLETED)
- **Task 1.1.2**: Grammar Implementation (IN PROGRESS)
- **Task 1.1.5**: Effect Syntax Support (BLOCKED - waiting on this feature)

### External References

- Leex Documentation: Erlang/OTP official docs
- Algebraic Effects Research: Research document 1.01, 1.07, 1.08
- Effect Systems: Koka, Eff, Frank languages (comparative analysis)
